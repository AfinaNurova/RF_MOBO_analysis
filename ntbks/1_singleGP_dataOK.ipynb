{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OKXolO-ZLw5"
   },
   "source": [
    "# Setting up the working env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9899,
     "status": "ok",
     "timestamp": 1744972328984,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "MFjw9sj6-rbo"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1823,
     "status": "ok",
     "timestamp": 1744972449383,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "VX0ORy94k5sO"
   },
   "outputs": [],
   "source": [
    "from botorch.test_functions.multi_objective import BraninCurrin\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
    "    FastNondominatedPartitioning,\n",
    ")\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qExpectedHypervolumeImprovement,\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1744972449452,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "7fYWouQx96Fp"
   },
   "outputs": [],
   "source": [
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 42  # or any integer you like\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjLcEUWYZR75"
   },
   "source": [
    "# Creating the Problem statement and initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqoye7dIBh9i"
   },
   "source": [
    "## Loading ADNIMERGE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2058,
     "status": "ok",
     "timestamp": 1744972534880,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "967dfa31-0559-4195-a1f9-6fc0ea060143",
    "outputId": "1fd0fbfa-f91b-45ec-caf2-70897d789098",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DX_bl</th>\n",
       "      <th>RID</th>\n",
       "      <th>ADAS13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN</td>\n",
       "      <td>2</td>\n",
       "      <td>18.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AD</td>\n",
       "      <td>3</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>4</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CN</td>\n",
       "      <td>5</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>6</td>\n",
       "      <td>25.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DX_bl  RID  ADAS13\n",
       "0     CN    2   18.67\n",
       "1     AD    3   31.00\n",
       "5   LMCI    4   21.33\n",
       "10    CN    5   14.67\n",
       "15  LMCI    6   25.67"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: PROBAR PROBLEMA BINARIO ASI EMCI LMCI\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "adnimerge = pd.read_parquet('../data/reduced_baseline_adnimerge_df.parquet', 'pyarrow')\n",
    "adnimerge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1744972534916,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "6c4adc19-c22d-4e63-ac4b-ad4c4f78e203",
    "outputId": "10216bae-7156-4977-8f8b-1165b7a6fa66",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DX_bl     12\n",
       "RID        0\n",
       "ADAS13    25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adnimerge.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744972534922,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "72ca4a77-e4de-426e-b37c-a0af58ad960c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "adnimerge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1744972534983,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "c37e2d6a-0483-4007-8d34-1db8b84e4832",
    "outputId": "8cfd88b1-15ae-4b0e-acd4-0e2fdf2b3d9b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adnimerge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1744972534987,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "L6L2vwKH7byv",
    "outputId": "7011fa9c-b397-4517-b22c-4f61c35241b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DX_bl      object\n",
       "RID         int64\n",
       "ADAS13    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adnimerge.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1744972535018,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "vZklCCnb7eqZ"
   },
   "outputs": [],
   "source": [
    "adnimerge['DX_bl'] = adnimerge['DX_bl'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1744972535063,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "tHueSZBH6fj9",
    "outputId": "ca9fa955-2343-4e0f-e553-1e8d013455e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StringArray>\n",
       "['CN', 'AD', 'LMCI', 'SMC', 'EMCI']\n",
       "Length: 5, dtype: string"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adnimerge['DX_bl'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa-70tK2Z_W-"
   },
   "source": [
    "### Creating the subset to be used (binary vs multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1744972535084,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "42ecd841-dfe1-432d-90d9-9e69765f2acc",
    "outputId": "cc1291ff-7638-42aa-faf2-c3dff397074c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>DX_bl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>LMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>LMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>38</td>\n",
       "      <td>LMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>42</td>\n",
       "      <td>LMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>45</td>\n",
       "      <td>LMCI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RID DX_bl\n",
       "5     4  LMCI\n",
       "15    6  LMCI\n",
       "61   38  LMCI\n",
       "69   42  LMCI\n",
       "84   45  LMCI"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adnimerge_sub = adnimerge[adnimerge['DX_bl'].isin(['LMCI', 'EMCI'])]\n",
    "adnimerge_sub = adnimerge_sub[['RID', 'DX_bl']]\n",
    "adnimerge_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1744972535120,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "wph7ZRl48Dum",
    "outputId": "56e62654-2852-40b7-ed94-77c65066ee8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1103, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adnimerge_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3589,
     "status": "ok",
     "timestamp": 1744972538712,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "0f89a55d-2e49-48ae-bf33-4e0db0c573c8",
    "outputId": "fd4b75c4-396d-46d0-e104-631ee6ede96b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1074, 154)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "roi_aparc = pd.read_parquet('../data/baseline_roi_aparc_df.parquet')\n",
    "roi_aparc['RID'] = roi_aparc['RID'].astype(int)\n",
    "roi_aparc.drop(columns=['subject', 'session_number'], inplace=True)\n",
    "(roi_aparc.head())\n",
    "data = adnimerge_sub.merge(roi_aparc, on='RID', how='inner')\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1744972538713,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "a094fc40-7e1e-4d29-89a8-51d63ff4c849",
    "outputId": "4ada1327-2d48-4a89-8b95-8f420b070664",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'DX_bl', 'lUnknown', 'rUnknown', 'lG_and_S_frontomargin',\n",
       "       'rG_and_S_frontomargin', 'lG_and_S_occipital_inf',\n",
       "       'rG_and_S_occipital_inf', 'lG_and_S_paracentral',\n",
       "       'rG_and_S_paracentral',\n",
       "       ...\n",
       "       'lS_suborbital', 'rS_suborbital', 'lS_subparietal', 'rS_subparietal',\n",
       "       'lS_temporal_inf', 'rS_temporal_inf', 'lS_temporal_sup',\n",
       "       'rS_temporal_sup', 'lS_temporal_transverse', 'rS_temporal_transverse'],\n",
       "      dtype='object', length=154)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1744972538831,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "5f322399-663a-4db3-8ac8-dec001de8213",
    "outputId": "be8117d4-9240-4b2b-b943-1e5794d99a64",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>DX_bl</th>\n",
       "      <th>lUnknown</th>\n",
       "      <th>rUnknown</th>\n",
       "      <th>lG_and_S_frontomargin</th>\n",
       "      <th>rG_and_S_frontomargin</th>\n",
       "      <th>lG_and_S_occipital_inf</th>\n",
       "      <th>rG_and_S_occipital_inf</th>\n",
       "      <th>lG_and_S_paracentral</th>\n",
       "      <th>rG_and_S_paracentral</th>\n",
       "      <th>lG_and_S_subcentral</th>\n",
       "      <th>rG_and_S_subcentral</th>\n",
       "      <th>lG_and_S_transv_frontopol</th>\n",
       "      <th>rG_and_S_transv_frontopol</th>\n",
       "      <th>lG_and_S_cingul-Ant</th>\n",
       "      <th>rG_and_S_cingul-Ant</th>\n",
       "      <th>lG_and_S_cingul-Mid-Ant</th>\n",
       "      <th>rG_and_S_cingul-Mid-Ant</th>\n",
       "      <th>lG_and_S_cingul-Mid-Post</th>\n",
       "      <th>rG_and_S_cingul-Mid-Post</th>\n",
       "      <th>lG_cingul-Post-dorsal</th>\n",
       "      <th>rG_cingul-Post-dorsal</th>\n",
       "      <th>lG_cingul-Post-ventral</th>\n",
       "      <th>rG_cingul-Post-ventral</th>\n",
       "      <th>lG_cuneus</th>\n",
       "      <th>rG_cuneus</th>\n",
       "      <th>lG_front_inf-Opercular</th>\n",
       "      <th>rG_front_inf-Opercular</th>\n",
       "      <th>lG_front_inf-Orbital</th>\n",
       "      <th>rG_front_inf-Orbital</th>\n",
       "      <th>lG_front_inf-Triangul</th>\n",
       "      <th>rG_front_inf-Triangul</th>\n",
       "      <th>lG_front_middle</th>\n",
       "      <th>rG_front_middle</th>\n",
       "      <th>lG_front_sup</th>\n",
       "      <th>rG_front_sup</th>\n",
       "      <th>lG_Ins_lg_and_S_cent_ins</th>\n",
       "      <th>rG_Ins_lg_and_S_cent_ins</th>\n",
       "      <th>lG_insular_short</th>\n",
       "      <th>rG_insular_short</th>\n",
       "      <th>lG_occipital_middle</th>\n",
       "      <th>rG_occipital_middle</th>\n",
       "      <th>lG_occipital_sup</th>\n",
       "      <th>rG_occipital_sup</th>\n",
       "      <th>lG_oc-temp_lat-fusifor</th>\n",
       "      <th>rG_oc-temp_lat-fusifor</th>\n",
       "      <th>lG_oc-temp_med-Lingual</th>\n",
       "      <th>rG_oc-temp_med-Lingual</th>\n",
       "      <th>lG_oc-temp_med-Parahip</th>\n",
       "      <th>rG_oc-temp_med-Parahip</th>\n",
       "      <th>lG_orbital</th>\n",
       "      <th>rG_orbital</th>\n",
       "      <th>lG_pariet_inf-Angular</th>\n",
       "      <th>rG_pariet_inf-Angular</th>\n",
       "      <th>lG_pariet_inf-Supramar</th>\n",
       "      <th>rG_pariet_inf-Supramar</th>\n",
       "      <th>lG_parietal_sup</th>\n",
       "      <th>rG_parietal_sup</th>\n",
       "      <th>lG_postcentral</th>\n",
       "      <th>rG_postcentral</th>\n",
       "      <th>lG_precentral</th>\n",
       "      <th>rG_precentral</th>\n",
       "      <th>lG_precuneus</th>\n",
       "      <th>rG_precuneus</th>\n",
       "      <th>lG_rectus</th>\n",
       "      <th>rG_rectus</th>\n",
       "      <th>lG_subcallosal</th>\n",
       "      <th>rG_subcallosal</th>\n",
       "      <th>lG_temp_sup-G_T_transv</th>\n",
       "      <th>rG_temp_sup-G_T_transv</th>\n",
       "      <th>lG_temp_sup-Lateral</th>\n",
       "      <th>rG_temp_sup-Lateral</th>\n",
       "      <th>lG_temp_sup-Plan_polar</th>\n",
       "      <th>rG_temp_sup-Plan_polar</th>\n",
       "      <th>lG_temp_sup-Plan_tempo</th>\n",
       "      <th>rG_temp_sup-Plan_tempo</th>\n",
       "      <th>lG_temporal_inf</th>\n",
       "      <th>rG_temporal_inf</th>\n",
       "      <th>lG_temporal_middle</th>\n",
       "      <th>rG_temporal_middle</th>\n",
       "      <th>lLat_Fis-ant-Horizont</th>\n",
       "      <th>rLat_Fis-ant-Horizont</th>\n",
       "      <th>lLat_Fis-ant-Vertical</th>\n",
       "      <th>rLat_Fis-ant-Vertical</th>\n",
       "      <th>lLat_Fis-post</th>\n",
       "      <th>rLat_Fis-post</th>\n",
       "      <th>lMedial_wall</th>\n",
       "      <th>rMedial_wall</th>\n",
       "      <th>lPole_occipital</th>\n",
       "      <th>rPole_occipital</th>\n",
       "      <th>lPole_temporal</th>\n",
       "      <th>rPole_temporal</th>\n",
       "      <th>lS_calcarine</th>\n",
       "      <th>rS_calcarine</th>\n",
       "      <th>lS_central</th>\n",
       "      <th>rS_central</th>\n",
       "      <th>lS_cingul-Marginalis</th>\n",
       "      <th>rS_cingul-Marginalis</th>\n",
       "      <th>lS_circular_insula_ant</th>\n",
       "      <th>rS_circular_insula_ant</th>\n",
       "      <th>lS_circular_insula_inf</th>\n",
       "      <th>rS_circular_insula_inf</th>\n",
       "      <th>lS_circular_insula_sup</th>\n",
       "      <th>rS_circular_insula_sup</th>\n",
       "      <th>lS_collat_transv_ant</th>\n",
       "      <th>rS_collat_transv_ant</th>\n",
       "      <th>lS_collat_transv_post</th>\n",
       "      <th>rS_collat_transv_post</th>\n",
       "      <th>lS_front_inf</th>\n",
       "      <th>rS_front_inf</th>\n",
       "      <th>lS_front_middle</th>\n",
       "      <th>rS_front_middle</th>\n",
       "      <th>lS_front_sup</th>\n",
       "      <th>rS_front_sup</th>\n",
       "      <th>lS_interm_prim-Jensen</th>\n",
       "      <th>rS_interm_prim-Jensen</th>\n",
       "      <th>lS_intrapariet_and_P_trans</th>\n",
       "      <th>rS_intrapariet_and_P_trans</th>\n",
       "      <th>lS_oc_middle_and_Lunatus</th>\n",
       "      <th>rS_oc_middle_and_Lunatus</th>\n",
       "      <th>lS_oc_sup_and_transversal</th>\n",
       "      <th>rS_oc_sup_and_transversal</th>\n",
       "      <th>lS_occipital_ant</th>\n",
       "      <th>rS_occipital_ant</th>\n",
       "      <th>lS_oc-temp_lat</th>\n",
       "      <th>rS_oc-temp_lat</th>\n",
       "      <th>lS_oc-temp_med_and_Lingual</th>\n",
       "      <th>rS_oc-temp_med_and_Lingual</th>\n",
       "      <th>lS_orbital_lateral</th>\n",
       "      <th>rS_orbital_lateral</th>\n",
       "      <th>lS_orbital_med-olfact</th>\n",
       "      <th>rS_orbital_med-olfact</th>\n",
       "      <th>lS_orbital-H_Shaped</th>\n",
       "      <th>rS_orbital-H_Shaped</th>\n",
       "      <th>lS_parieto_occipital</th>\n",
       "      <th>rS_parieto_occipital</th>\n",
       "      <th>lS_pericallosal</th>\n",
       "      <th>rS_pericallosal</th>\n",
       "      <th>lS_postcentral</th>\n",
       "      <th>rS_postcentral</th>\n",
       "      <th>lS_precentral-inf-part</th>\n",
       "      <th>rS_precentral-inf-part</th>\n",
       "      <th>lS_precentral-sup-part</th>\n",
       "      <th>rS_precentral-sup-part</th>\n",
       "      <th>lS_suborbital</th>\n",
       "      <th>rS_suborbital</th>\n",
       "      <th>lS_subparietal</th>\n",
       "      <th>rS_subparietal</th>\n",
       "      <th>lS_temporal_inf</th>\n",
       "      <th>rS_temporal_inf</th>\n",
       "      <th>lS_temporal_sup</th>\n",
       "      <th>rS_temporal_sup</th>\n",
       "      <th>lS_temporal_transverse</th>\n",
       "      <th>rS_temporal_transverse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>1.986359</td>\n",
       "      <td>1.926012</td>\n",
       "      <td>2.235178</td>\n",
       "      <td>2.199780</td>\n",
       "      <td>2.263434</td>\n",
       "      <td>2.586857</td>\n",
       "      <td>2.051794</td>\n",
       "      <td>1.902948</td>\n",
       "      <td>2.314763</td>\n",
       "      <td>2.445275</td>\n",
       "      <td>2.276910</td>\n",
       "      <td>2.352085</td>\n",
       "      <td>2.504892</td>\n",
       "      <td>2.352772</td>\n",
       "      <td>2.428664</td>\n",
       "      <td>2.321035</td>\n",
       "      <td>2.397175</td>\n",
       "      <td>2.225512</td>\n",
       "      <td>2.591127</td>\n",
       "      <td>2.649444</td>\n",
       "      <td>2.377577</td>\n",
       "      <td>2.628912</td>\n",
       "      <td>1.745167</td>\n",
       "      <td>1.629821</td>\n",
       "      <td>2.553475</td>\n",
       "      <td>2.503641</td>\n",
       "      <td>2.588122</td>\n",
       "      <td>2.493712</td>\n",
       "      <td>2.239265</td>\n",
       "      <td>2.354536</td>\n",
       "      <td>2.330980</td>\n",
       "      <td>2.391344</td>\n",
       "      <td>2.521309</td>\n",
       "      <td>2.431288</td>\n",
       "      <td>3.281339</td>\n",
       "      <td>3.034438</td>\n",
       "      <td>3.676079</td>\n",
       "      <td>3.898588</td>\n",
       "      <td>2.257806</td>\n",
       "      <td>2.326474</td>\n",
       "      <td>2.082201</td>\n",
       "      <td>1.964689</td>\n",
       "      <td>2.331578</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>1.842906</td>\n",
       "      <td>1.933253</td>\n",
       "      <td>2.921091</td>\n",
       "      <td>3.126393</td>\n",
       "      <td>2.695209</td>\n",
       "      <td>2.805978</td>\n",
       "      <td>2.422602</td>\n",
       "      <td>2.501791</td>\n",
       "      <td>2.411938</td>\n",
       "      <td>2.372590</td>\n",
       "      <td>2.196704</td>\n",
       "      <td>2.141227</td>\n",
       "      <td>1.978632</td>\n",
       "      <td>2.078509</td>\n",
       "      <td>1.924300</td>\n",
       "      <td>2.039493</td>\n",
       "      <td>2.372580</td>\n",
       "      <td>2.374773</td>\n",
       "      <td>2.291711</td>\n",
       "      <td>2.495232</td>\n",
       "      <td>2.543986</td>\n",
       "      <td>3.220273</td>\n",
       "      <td>1.908910</td>\n",
       "      <td>2.289189</td>\n",
       "      <td>2.722966</td>\n",
       "      <td>2.812194</td>\n",
       "      <td>3.404710</td>\n",
       "      <td>3.642868</td>\n",
       "      <td>2.352714</td>\n",
       "      <td>2.375830</td>\n",
       "      <td>2.681285</td>\n",
       "      <td>2.757351</td>\n",
       "      <td>2.710977</td>\n",
       "      <td>2.849796</td>\n",
       "      <td>2.299123</td>\n",
       "      <td>2.367012</td>\n",
       "      <td>2.341449</td>\n",
       "      <td>2.343801</td>\n",
       "      <td>1.968207</td>\n",
       "      <td>2.403490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.831361</td>\n",
       "      <td>1.925835</td>\n",
       "      <td>3.106892</td>\n",
       "      <td>3.173338</td>\n",
       "      <td>1.787686</td>\n",
       "      <td>2.043565</td>\n",
       "      <td>1.595498</td>\n",
       "      <td>1.661827</td>\n",
       "      <td>2.404563</td>\n",
       "      <td>2.308197</td>\n",
       "      <td>3.458154</td>\n",
       "      <td>3.415487</td>\n",
       "      <td>3.000959</td>\n",
       "      <td>3.066050</td>\n",
       "      <td>2.725759</td>\n",
       "      <td>2.919006</td>\n",
       "      <td>2.802477</td>\n",
       "      <td>2.654570</td>\n",
       "      <td>1.822014</td>\n",
       "      <td>1.979481</td>\n",
       "      <td>2.138143</td>\n",
       "      <td>2.206768</td>\n",
       "      <td>2.186763</td>\n",
       "      <td>2.219792</td>\n",
       "      <td>2.522033</td>\n",
       "      <td>2.359838</td>\n",
       "      <td>2.301323</td>\n",
       "      <td>2.402576</td>\n",
       "      <td>2.041849</td>\n",
       "      <td>2.140886</td>\n",
       "      <td>2.015811</td>\n",
       "      <td>1.957807</td>\n",
       "      <td>2.070450</td>\n",
       "      <td>1.968242</td>\n",
       "      <td>2.163826</td>\n",
       "      <td>2.058096</td>\n",
       "      <td>2.493632</td>\n",
       "      <td>2.539819</td>\n",
       "      <td>2.240415</td>\n",
       "      <td>2.370086</td>\n",
       "      <td>2.217716</td>\n",
       "      <td>2.062718</td>\n",
       "      <td>2.266284</td>\n",
       "      <td>2.568335</td>\n",
       "      <td>2.498707</td>\n",
       "      <td>2.477320</td>\n",
       "      <td>2.001671</td>\n",
       "      <td>2.098518</td>\n",
       "      <td>1.807530</td>\n",
       "      <td>1.877744</td>\n",
       "      <td>2.094920</td>\n",
       "      <td>2.176244</td>\n",
       "      <td>2.394351</td>\n",
       "      <td>2.386774</td>\n",
       "      <td>2.119276</td>\n",
       "      <td>2.155038</td>\n",
       "      <td>2.234450</td>\n",
       "      <td>2.244920</td>\n",
       "      <td>2.282227</td>\n",
       "      <td>2.256248</td>\n",
       "      <td>2.373447</td>\n",
       "      <td>2.439594</td>\n",
       "      <td>2.442846</td>\n",
       "      <td>2.418286</td>\n",
       "      <td>1.946976</td>\n",
       "      <td>1.929366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>1.474435</td>\n",
       "      <td>1.472652</td>\n",
       "      <td>2.519347</td>\n",
       "      <td>2.464332</td>\n",
       "      <td>2.294019</td>\n",
       "      <td>2.311054</td>\n",
       "      <td>1.526549</td>\n",
       "      <td>1.318640</td>\n",
       "      <td>2.418625</td>\n",
       "      <td>2.379175</td>\n",
       "      <td>2.446642</td>\n",
       "      <td>2.420569</td>\n",
       "      <td>2.657760</td>\n",
       "      <td>2.462927</td>\n",
       "      <td>2.479504</td>\n",
       "      <td>2.646902</td>\n",
       "      <td>2.482917</td>\n",
       "      <td>2.414501</td>\n",
       "      <td>2.630887</td>\n",
       "      <td>2.593843</td>\n",
       "      <td>2.350999</td>\n",
       "      <td>2.678580</td>\n",
       "      <td>1.462866</td>\n",
       "      <td>1.527411</td>\n",
       "      <td>2.725389</td>\n",
       "      <td>2.761860</td>\n",
       "      <td>2.554240</td>\n",
       "      <td>2.432371</td>\n",
       "      <td>2.445395</td>\n",
       "      <td>2.401267</td>\n",
       "      <td>2.489807</td>\n",
       "      <td>2.386696</td>\n",
       "      <td>2.466984</td>\n",
       "      <td>2.568141</td>\n",
       "      <td>3.100075</td>\n",
       "      <td>2.620097</td>\n",
       "      <td>3.565704</td>\n",
       "      <td>3.082581</td>\n",
       "      <td>2.508954</td>\n",
       "      <td>2.523208</td>\n",
       "      <td>1.954296</td>\n",
       "      <td>1.911302</td>\n",
       "      <td>2.700320</td>\n",
       "      <td>2.606456</td>\n",
       "      <td>1.628881</td>\n",
       "      <td>1.708448</td>\n",
       "      <td>2.466486</td>\n",
       "      <td>2.546927</td>\n",
       "      <td>2.686398</td>\n",
       "      <td>2.659456</td>\n",
       "      <td>2.553232</td>\n",
       "      <td>2.422354</td>\n",
       "      <td>2.465979</td>\n",
       "      <td>2.571069</td>\n",
       "      <td>2.365390</td>\n",
       "      <td>2.202038</td>\n",
       "      <td>1.711097</td>\n",
       "      <td>1.457784</td>\n",
       "      <td>2.171960</td>\n",
       "      <td>2.194586</td>\n",
       "      <td>2.333405</td>\n",
       "      <td>2.318397</td>\n",
       "      <td>2.309580</td>\n",
       "      <td>2.220330</td>\n",
       "      <td>3.316470</td>\n",
       "      <td>2.589482</td>\n",
       "      <td>2.443934</td>\n",
       "      <td>2.436524</td>\n",
       "      <td>2.769449</td>\n",
       "      <td>2.912509</td>\n",
       "      <td>2.966688</td>\n",
       "      <td>2.610880</td>\n",
       "      <td>2.444941</td>\n",
       "      <td>2.479290</td>\n",
       "      <td>2.640138</td>\n",
       "      <td>2.710535</td>\n",
       "      <td>2.662709</td>\n",
       "      <td>2.805806</td>\n",
       "      <td>2.382342</td>\n",
       "      <td>2.431699</td>\n",
       "      <td>2.346092</td>\n",
       "      <td>2.546148</td>\n",
       "      <td>2.255039</td>\n",
       "      <td>2.351424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.796661</td>\n",
       "      <td>1.757166</td>\n",
       "      <td>3.112608</td>\n",
       "      <td>2.920239</td>\n",
       "      <td>1.595840</td>\n",
       "      <td>1.686719</td>\n",
       "      <td>1.347035</td>\n",
       "      <td>1.287529</td>\n",
       "      <td>2.302328</td>\n",
       "      <td>2.054686</td>\n",
       "      <td>3.146097</td>\n",
       "      <td>2.909812</td>\n",
       "      <td>2.743656</td>\n",
       "      <td>2.612447</td>\n",
       "      <td>2.633974</td>\n",
       "      <td>2.751714</td>\n",
       "      <td>2.569960</td>\n",
       "      <td>2.604781</td>\n",
       "      <td>2.022331</td>\n",
       "      <td>2.178153</td>\n",
       "      <td>2.318094</td>\n",
       "      <td>2.396334</td>\n",
       "      <td>2.272284</td>\n",
       "      <td>2.327795</td>\n",
       "      <td>2.473423</td>\n",
       "      <td>2.671409</td>\n",
       "      <td>2.279328</td>\n",
       "      <td>2.424174</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.185688</td>\n",
       "      <td>2.130158</td>\n",
       "      <td>2.180025</td>\n",
       "      <td>2.128435</td>\n",
       "      <td>2.119946</td>\n",
       "      <td>2.033292</td>\n",
       "      <td>2.112388</td>\n",
       "      <td>2.331296</td>\n",
       "      <td>2.222434</td>\n",
       "      <td>2.181254</td>\n",
       "      <td>2.099374</td>\n",
       "      <td>2.178623</td>\n",
       "      <td>2.452816</td>\n",
       "      <td>2.211108</td>\n",
       "      <td>2.258118</td>\n",
       "      <td>2.417704</td>\n",
       "      <td>2.541279</td>\n",
       "      <td>1.965445</td>\n",
       "      <td>2.077335</td>\n",
       "      <td>1.817071</td>\n",
       "      <td>1.947190</td>\n",
       "      <td>2.128129</td>\n",
       "      <td>1.878907</td>\n",
       "      <td>2.318670</td>\n",
       "      <td>2.412602</td>\n",
       "      <td>2.159158</td>\n",
       "      <td>2.310238</td>\n",
       "      <td>2.445196</td>\n",
       "      <td>2.248049</td>\n",
       "      <td>2.333428</td>\n",
       "      <td>2.327283</td>\n",
       "      <td>2.484239</td>\n",
       "      <td>2.456764</td>\n",
       "      <td>2.430170</td>\n",
       "      <td>2.508620</td>\n",
       "      <td>2.172205</td>\n",
       "      <td>2.547939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>1.618434</td>\n",
       "      <td>1.820242</td>\n",
       "      <td>2.167348</td>\n",
       "      <td>2.164652</td>\n",
       "      <td>2.292630</td>\n",
       "      <td>2.366592</td>\n",
       "      <td>1.847092</td>\n",
       "      <td>1.734667</td>\n",
       "      <td>2.411643</td>\n",
       "      <td>2.510770</td>\n",
       "      <td>2.211827</td>\n",
       "      <td>2.309805</td>\n",
       "      <td>2.682115</td>\n",
       "      <td>2.619095</td>\n",
       "      <td>2.568234</td>\n",
       "      <td>2.350013</td>\n",
       "      <td>2.338628</td>\n",
       "      <td>2.350377</td>\n",
       "      <td>2.787651</td>\n",
       "      <td>3.166329</td>\n",
       "      <td>2.177903</td>\n",
       "      <td>2.561018</td>\n",
       "      <td>1.703072</td>\n",
       "      <td>1.682870</td>\n",
       "      <td>2.549385</td>\n",
       "      <td>2.560862</td>\n",
       "      <td>2.739262</td>\n",
       "      <td>2.348159</td>\n",
       "      <td>2.400468</td>\n",
       "      <td>2.351866</td>\n",
       "      <td>2.265144</td>\n",
       "      <td>2.343004</td>\n",
       "      <td>2.445462</td>\n",
       "      <td>2.425785</td>\n",
       "      <td>3.579311</td>\n",
       "      <td>3.203484</td>\n",
       "      <td>4.116234</td>\n",
       "      <td>3.956138</td>\n",
       "      <td>2.340168</td>\n",
       "      <td>2.404881</td>\n",
       "      <td>2.062779</td>\n",
       "      <td>2.139359</td>\n",
       "      <td>2.735869</td>\n",
       "      <td>2.530499</td>\n",
       "      <td>1.799470</td>\n",
       "      <td>1.901053</td>\n",
       "      <td>3.054599</td>\n",
       "      <td>3.155449</td>\n",
       "      <td>2.519554</td>\n",
       "      <td>2.654329</td>\n",
       "      <td>2.445448</td>\n",
       "      <td>2.538123</td>\n",
       "      <td>2.313413</td>\n",
       "      <td>2.547944</td>\n",
       "      <td>2.200662</td>\n",
       "      <td>2.221020</td>\n",
       "      <td>1.833015</td>\n",
       "      <td>1.793619</td>\n",
       "      <td>2.074219</td>\n",
       "      <td>2.109261</td>\n",
       "      <td>2.279788</td>\n",
       "      <td>2.402283</td>\n",
       "      <td>2.401778</td>\n",
       "      <td>2.445999</td>\n",
       "      <td>3.251933</td>\n",
       "      <td>2.980795</td>\n",
       "      <td>2.090480</td>\n",
       "      <td>2.134567</td>\n",
       "      <td>2.621562</td>\n",
       "      <td>2.783181</td>\n",
       "      <td>3.257760</td>\n",
       "      <td>3.462884</td>\n",
       "      <td>2.105893</td>\n",
       "      <td>2.402274</td>\n",
       "      <td>2.605612</td>\n",
       "      <td>2.724318</td>\n",
       "      <td>2.650980</td>\n",
       "      <td>2.749851</td>\n",
       "      <td>2.507766</td>\n",
       "      <td>2.280558</td>\n",
       "      <td>2.498169</td>\n",
       "      <td>2.433188</td>\n",
       "      <td>2.162759</td>\n",
       "      <td>2.480639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.892133</td>\n",
       "      <td>1.950294</td>\n",
       "      <td>3.021313</td>\n",
       "      <td>3.078840</td>\n",
       "      <td>1.692471</td>\n",
       "      <td>1.744289</td>\n",
       "      <td>1.518490</td>\n",
       "      <td>1.487509</td>\n",
       "      <td>2.065293</td>\n",
       "      <td>2.098563</td>\n",
       "      <td>3.169226</td>\n",
       "      <td>3.262496</td>\n",
       "      <td>2.921727</td>\n",
       "      <td>2.701621</td>\n",
       "      <td>2.843148</td>\n",
       "      <td>3.028769</td>\n",
       "      <td>2.516392</td>\n",
       "      <td>2.509358</td>\n",
       "      <td>1.986084</td>\n",
       "      <td>1.794813</td>\n",
       "      <td>2.166686</td>\n",
       "      <td>2.148551</td>\n",
       "      <td>2.151709</td>\n",
       "      <td>2.130060</td>\n",
       "      <td>2.317211</td>\n",
       "      <td>2.245349</td>\n",
       "      <td>2.096711</td>\n",
       "      <td>2.178812</td>\n",
       "      <td>2.115657</td>\n",
       "      <td>2.154094</td>\n",
       "      <td>2.114987</td>\n",
       "      <td>2.057267</td>\n",
       "      <td>2.020059</td>\n",
       "      <td>2.015122</td>\n",
       "      <td>2.285404</td>\n",
       "      <td>2.139183</td>\n",
       "      <td>2.556137</td>\n",
       "      <td>2.513475</td>\n",
       "      <td>2.365632</td>\n",
       "      <td>2.628261</td>\n",
       "      <td>2.409998</td>\n",
       "      <td>2.113767</td>\n",
       "      <td>2.408826</td>\n",
       "      <td>2.536114</td>\n",
       "      <td>2.489287</td>\n",
       "      <td>2.528121</td>\n",
       "      <td>2.020478</td>\n",
       "      <td>2.130425</td>\n",
       "      <td>1.800021</td>\n",
       "      <td>1.567593</td>\n",
       "      <td>1.927498</td>\n",
       "      <td>1.941317</td>\n",
       "      <td>2.427749</td>\n",
       "      <td>2.326529</td>\n",
       "      <td>2.153913</td>\n",
       "      <td>2.119882</td>\n",
       "      <td>2.608098</td>\n",
       "      <td>2.283140</td>\n",
       "      <td>2.366019</td>\n",
       "      <td>2.215158</td>\n",
       "      <td>2.330570</td>\n",
       "      <td>2.437196</td>\n",
       "      <td>2.386827</td>\n",
       "      <td>2.519870</td>\n",
       "      <td>1.931591</td>\n",
       "      <td>2.130694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>1.617035</td>\n",
       "      <td>1.631853</td>\n",
       "      <td>1.998385</td>\n",
       "      <td>2.108666</td>\n",
       "      <td>2.354013</td>\n",
       "      <td>2.377864</td>\n",
       "      <td>2.239820</td>\n",
       "      <td>2.051575</td>\n",
       "      <td>2.231758</td>\n",
       "      <td>2.129171</td>\n",
       "      <td>2.048007</td>\n",
       "      <td>1.938442</td>\n",
       "      <td>2.423190</td>\n",
       "      <td>2.379568</td>\n",
       "      <td>2.290642</td>\n",
       "      <td>2.455297</td>\n",
       "      <td>2.241113</td>\n",
       "      <td>2.651566</td>\n",
       "      <td>2.560461</td>\n",
       "      <td>2.531661</td>\n",
       "      <td>2.284608</td>\n",
       "      <td>2.566362</td>\n",
       "      <td>1.611983</td>\n",
       "      <td>1.689036</td>\n",
       "      <td>2.231706</td>\n",
       "      <td>2.331956</td>\n",
       "      <td>2.269613</td>\n",
       "      <td>2.079971</td>\n",
       "      <td>2.027859</td>\n",
       "      <td>2.063547</td>\n",
       "      <td>2.299954</td>\n",
       "      <td>2.285489</td>\n",
       "      <td>2.491552</td>\n",
       "      <td>2.636851</td>\n",
       "      <td>3.077230</td>\n",
       "      <td>2.646450</td>\n",
       "      <td>3.761522</td>\n",
       "      <td>3.982027</td>\n",
       "      <td>2.428474</td>\n",
       "      <td>2.336983</td>\n",
       "      <td>1.956718</td>\n",
       "      <td>1.967924</td>\n",
       "      <td>2.730821</td>\n",
       "      <td>2.639524</td>\n",
       "      <td>1.859430</td>\n",
       "      <td>1.907578</td>\n",
       "      <td>2.460135</td>\n",
       "      <td>2.943835</td>\n",
       "      <td>2.386702</td>\n",
       "      <td>2.399509</td>\n",
       "      <td>2.321229</td>\n",
       "      <td>2.484544</td>\n",
       "      <td>2.328901</td>\n",
       "      <td>2.391663</td>\n",
       "      <td>2.389039</td>\n",
       "      <td>2.290712</td>\n",
       "      <td>2.033650</td>\n",
       "      <td>1.903333</td>\n",
       "      <td>2.356269</td>\n",
       "      <td>2.489546</td>\n",
       "      <td>2.367732</td>\n",
       "      <td>2.409797</td>\n",
       "      <td>2.210943</td>\n",
       "      <td>2.120643</td>\n",
       "      <td>2.858720</td>\n",
       "      <td>2.474946</td>\n",
       "      <td>1.815059</td>\n",
       "      <td>1.743792</td>\n",
       "      <td>2.733618</td>\n",
       "      <td>2.774414</td>\n",
       "      <td>2.809290</td>\n",
       "      <td>3.254334</td>\n",
       "      <td>2.446254</td>\n",
       "      <td>2.135171</td>\n",
       "      <td>2.704359</td>\n",
       "      <td>2.777295</td>\n",
       "      <td>2.444769</td>\n",
       "      <td>2.612276</td>\n",
       "      <td>2.063223</td>\n",
       "      <td>2.308239</td>\n",
       "      <td>2.130290</td>\n",
       "      <td>1.829609</td>\n",
       "      <td>1.979185</td>\n",
       "      <td>2.210194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.960188</td>\n",
       "      <td>1.950527</td>\n",
       "      <td>3.017652</td>\n",
       "      <td>3.085552</td>\n",
       "      <td>1.739287</td>\n",
       "      <td>1.918926</td>\n",
       "      <td>1.748103</td>\n",
       "      <td>1.674920</td>\n",
       "      <td>2.362947</td>\n",
       "      <td>2.336657</td>\n",
       "      <td>2.664091</td>\n",
       "      <td>2.780160</td>\n",
       "      <td>2.393347</td>\n",
       "      <td>2.543878</td>\n",
       "      <td>2.382239</td>\n",
       "      <td>2.685439</td>\n",
       "      <td>2.508114</td>\n",
       "      <td>2.734307</td>\n",
       "      <td>1.976895</td>\n",
       "      <td>1.968817</td>\n",
       "      <td>2.132622</td>\n",
       "      <td>2.093264</td>\n",
       "      <td>2.098069</td>\n",
       "      <td>2.047001</td>\n",
       "      <td>2.459728</td>\n",
       "      <td>2.404702</td>\n",
       "      <td>2.053868</td>\n",
       "      <td>2.331705</td>\n",
       "      <td>2.156128</td>\n",
       "      <td>2.186819</td>\n",
       "      <td>2.240412</td>\n",
       "      <td>2.140361</td>\n",
       "      <td>2.273446</td>\n",
       "      <td>2.033141</td>\n",
       "      <td>2.026850</td>\n",
       "      <td>1.996814</td>\n",
       "      <td>2.479676</td>\n",
       "      <td>2.444856</td>\n",
       "      <td>2.286056</td>\n",
       "      <td>2.220625</td>\n",
       "      <td>1.950955</td>\n",
       "      <td>1.951106</td>\n",
       "      <td>2.183467</td>\n",
       "      <td>2.160477</td>\n",
       "      <td>2.562018</td>\n",
       "      <td>2.718355</td>\n",
       "      <td>1.927688</td>\n",
       "      <td>1.931133</td>\n",
       "      <td>1.605575</td>\n",
       "      <td>1.804221</td>\n",
       "      <td>2.043672</td>\n",
       "      <td>2.148453</td>\n",
       "      <td>2.238210</td>\n",
       "      <td>2.145806</td>\n",
       "      <td>2.440251</td>\n",
       "      <td>2.351300</td>\n",
       "      <td>2.126382</td>\n",
       "      <td>2.036968</td>\n",
       "      <td>2.238736</td>\n",
       "      <td>2.386632</td>\n",
       "      <td>2.442251</td>\n",
       "      <td>2.331181</td>\n",
       "      <td>2.252620</td>\n",
       "      <td>2.375511</td>\n",
       "      <td>1.757637</td>\n",
       "      <td>1.856733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>1.639067</td>\n",
       "      <td>1.637800</td>\n",
       "      <td>2.277544</td>\n",
       "      <td>2.409265</td>\n",
       "      <td>2.275624</td>\n",
       "      <td>2.535532</td>\n",
       "      <td>1.554667</td>\n",
       "      <td>1.614461</td>\n",
       "      <td>2.486418</td>\n",
       "      <td>2.419577</td>\n",
       "      <td>2.026369</td>\n",
       "      <td>1.953978</td>\n",
       "      <td>2.624747</td>\n",
       "      <td>2.509418</td>\n",
       "      <td>2.556768</td>\n",
       "      <td>2.426639</td>\n",
       "      <td>2.364494</td>\n",
       "      <td>2.184326</td>\n",
       "      <td>2.476001</td>\n",
       "      <td>2.703863</td>\n",
       "      <td>1.787875</td>\n",
       "      <td>2.400229</td>\n",
       "      <td>1.624513</td>\n",
       "      <td>1.709165</td>\n",
       "      <td>2.397410</td>\n",
       "      <td>2.600635</td>\n",
       "      <td>2.583486</td>\n",
       "      <td>2.361261</td>\n",
       "      <td>2.227988</td>\n",
       "      <td>2.256709</td>\n",
       "      <td>2.171939</td>\n",
       "      <td>2.198085</td>\n",
       "      <td>2.292109</td>\n",
       "      <td>2.215032</td>\n",
       "      <td>3.089306</td>\n",
       "      <td>2.673509</td>\n",
       "      <td>3.275040</td>\n",
       "      <td>3.970294</td>\n",
       "      <td>2.479741</td>\n",
       "      <td>2.431082</td>\n",
       "      <td>1.927752</td>\n",
       "      <td>1.967944</td>\n",
       "      <td>2.320989</td>\n",
       "      <td>2.565484</td>\n",
       "      <td>1.750776</td>\n",
       "      <td>1.680309</td>\n",
       "      <td>2.337275</td>\n",
       "      <td>2.783960</td>\n",
       "      <td>2.771243</td>\n",
       "      <td>2.918862</td>\n",
       "      <td>2.198639</td>\n",
       "      <td>2.325711</td>\n",
       "      <td>2.363832</td>\n",
       "      <td>2.341019</td>\n",
       "      <td>2.012848</td>\n",
       "      <td>2.066757</td>\n",
       "      <td>1.803862</td>\n",
       "      <td>1.614757</td>\n",
       "      <td>1.902237</td>\n",
       "      <td>1.848689</td>\n",
       "      <td>2.205156</td>\n",
       "      <td>2.253479</td>\n",
       "      <td>2.235300</td>\n",
       "      <td>2.174727</td>\n",
       "      <td>2.433027</td>\n",
       "      <td>3.080570</td>\n",
       "      <td>1.752716</td>\n",
       "      <td>1.712989</td>\n",
       "      <td>2.410415</td>\n",
       "      <td>2.733909</td>\n",
       "      <td>2.722359</td>\n",
       "      <td>3.074588</td>\n",
       "      <td>2.343121</td>\n",
       "      <td>2.116158</td>\n",
       "      <td>2.363601</td>\n",
       "      <td>2.644726</td>\n",
       "      <td>2.457741</td>\n",
       "      <td>2.609281</td>\n",
       "      <td>2.296422</td>\n",
       "      <td>2.282857</td>\n",
       "      <td>2.078151</td>\n",
       "      <td>2.624582</td>\n",
       "      <td>2.244507</td>\n",
       "      <td>2.350743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.879614</td>\n",
       "      <td>2.051437</td>\n",
       "      <td>2.637061</td>\n",
       "      <td>2.989546</td>\n",
       "      <td>1.823235</td>\n",
       "      <td>1.760244</td>\n",
       "      <td>1.390253</td>\n",
       "      <td>1.339797</td>\n",
       "      <td>1.922822</td>\n",
       "      <td>2.001445</td>\n",
       "      <td>3.226001</td>\n",
       "      <td>3.236086</td>\n",
       "      <td>2.502783</td>\n",
       "      <td>2.582805</td>\n",
       "      <td>2.682365</td>\n",
       "      <td>2.965817</td>\n",
       "      <td>2.232335</td>\n",
       "      <td>2.365675</td>\n",
       "      <td>1.862728</td>\n",
       "      <td>2.039689</td>\n",
       "      <td>2.167444</td>\n",
       "      <td>2.209746</td>\n",
       "      <td>2.176439</td>\n",
       "      <td>2.120962</td>\n",
       "      <td>2.161039</td>\n",
       "      <td>2.224672</td>\n",
       "      <td>2.127076</td>\n",
       "      <td>1.985744</td>\n",
       "      <td>1.929475</td>\n",
       "      <td>1.891974</td>\n",
       "      <td>2.057004</td>\n",
       "      <td>2.197119</td>\n",
       "      <td>2.084604</td>\n",
       "      <td>2.078425</td>\n",
       "      <td>2.291279</td>\n",
       "      <td>2.269948</td>\n",
       "      <td>2.260950</td>\n",
       "      <td>2.479011</td>\n",
       "      <td>2.259284</td>\n",
       "      <td>2.420669</td>\n",
       "      <td>2.210238</td>\n",
       "      <td>2.341858</td>\n",
       "      <td>2.241286</td>\n",
       "      <td>2.376118</td>\n",
       "      <td>2.529711</td>\n",
       "      <td>2.588473</td>\n",
       "      <td>2.038059</td>\n",
       "      <td>2.150270</td>\n",
       "      <td>1.679060</td>\n",
       "      <td>1.503930</td>\n",
       "      <td>1.808304</td>\n",
       "      <td>1.784406</td>\n",
       "      <td>2.192276</td>\n",
       "      <td>2.057184</td>\n",
       "      <td>2.167805</td>\n",
       "      <td>1.942588</td>\n",
       "      <td>2.241874</td>\n",
       "      <td>2.002333</td>\n",
       "      <td>2.274040</td>\n",
       "      <td>2.126398</td>\n",
       "      <td>2.140593</td>\n",
       "      <td>2.405042</td>\n",
       "      <td>2.156671</td>\n",
       "      <td>2.309237</td>\n",
       "      <td>2.050010</td>\n",
       "      <td>1.831899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RID DX_bl  lUnknown  rUnknown  lG_and_S_frontomargin  \\\n",
       "0    4  LMCI  1.986359  1.926012               2.235178   \n",
       "1    6  LMCI  1.474435  1.472652               2.519347   \n",
       "2   38  LMCI  1.618434  1.820242               2.167348   \n",
       "3   42  LMCI  1.617035  1.631853               1.998385   \n",
       "4   45  LMCI  1.639067  1.637800               2.277544   \n",
       "\n",
       "   rG_and_S_frontomargin  lG_and_S_occipital_inf  rG_and_S_occipital_inf  \\\n",
       "0               2.199780                2.263434                2.586857   \n",
       "1               2.464332                2.294019                2.311054   \n",
       "2               2.164652                2.292630                2.366592   \n",
       "3               2.108666                2.354013                2.377864   \n",
       "4               2.409265                2.275624                2.535532   \n",
       "\n",
       "   lG_and_S_paracentral  rG_and_S_paracentral  lG_and_S_subcentral  \\\n",
       "0              2.051794              1.902948             2.314763   \n",
       "1              1.526549              1.318640             2.418625   \n",
       "2              1.847092              1.734667             2.411643   \n",
       "3              2.239820              2.051575             2.231758   \n",
       "4              1.554667              1.614461             2.486418   \n",
       "\n",
       "   rG_and_S_subcentral  lG_and_S_transv_frontopol  rG_and_S_transv_frontopol  \\\n",
       "0             2.445275                   2.276910                   2.352085   \n",
       "1             2.379175                   2.446642                   2.420569   \n",
       "2             2.510770                   2.211827                   2.309805   \n",
       "3             2.129171                   2.048007                   1.938442   \n",
       "4             2.419577                   2.026369                   1.953978   \n",
       "\n",
       "   lG_and_S_cingul-Ant  rG_and_S_cingul-Ant  lG_and_S_cingul-Mid-Ant  \\\n",
       "0             2.504892             2.352772                 2.428664   \n",
       "1             2.657760             2.462927                 2.479504   \n",
       "2             2.682115             2.619095                 2.568234   \n",
       "3             2.423190             2.379568                 2.290642   \n",
       "4             2.624747             2.509418                 2.556768   \n",
       "\n",
       "   rG_and_S_cingul-Mid-Ant  lG_and_S_cingul-Mid-Post  \\\n",
       "0                 2.321035                  2.397175   \n",
       "1                 2.646902                  2.482917   \n",
       "2                 2.350013                  2.338628   \n",
       "3                 2.455297                  2.241113   \n",
       "4                 2.426639                  2.364494   \n",
       "\n",
       "   rG_and_S_cingul-Mid-Post  lG_cingul-Post-dorsal  rG_cingul-Post-dorsal  \\\n",
       "0                  2.225512               2.591127               2.649444   \n",
       "1                  2.414501               2.630887               2.593843   \n",
       "2                  2.350377               2.787651               3.166329   \n",
       "3                  2.651566               2.560461               2.531661   \n",
       "4                  2.184326               2.476001               2.703863   \n",
       "\n",
       "   lG_cingul-Post-ventral  rG_cingul-Post-ventral  lG_cuneus  rG_cuneus  \\\n",
       "0                2.377577                2.628912   1.745167   1.629821   \n",
       "1                2.350999                2.678580   1.462866   1.527411   \n",
       "2                2.177903                2.561018   1.703072   1.682870   \n",
       "3                2.284608                2.566362   1.611983   1.689036   \n",
       "4                1.787875                2.400229   1.624513   1.709165   \n",
       "\n",
       "   lG_front_inf-Opercular  rG_front_inf-Opercular  lG_front_inf-Orbital  \\\n",
       "0                2.553475                2.503641              2.588122   \n",
       "1                2.725389                2.761860              2.554240   \n",
       "2                2.549385                2.560862              2.739262   \n",
       "3                2.231706                2.331956              2.269613   \n",
       "4                2.397410                2.600635              2.583486   \n",
       "\n",
       "   rG_front_inf-Orbital  lG_front_inf-Triangul  rG_front_inf-Triangul  \\\n",
       "0              2.493712               2.239265               2.354536   \n",
       "1              2.432371               2.445395               2.401267   \n",
       "2              2.348159               2.400468               2.351866   \n",
       "3              2.079971               2.027859               2.063547   \n",
       "4              2.361261               2.227988               2.256709   \n",
       "\n",
       "   lG_front_middle  rG_front_middle  lG_front_sup  rG_front_sup  \\\n",
       "0         2.330980         2.391344      2.521309      2.431288   \n",
       "1         2.489807         2.386696      2.466984      2.568141   \n",
       "2         2.265144         2.343004      2.445462      2.425785   \n",
       "3         2.299954         2.285489      2.491552      2.636851   \n",
       "4         2.171939         2.198085      2.292109      2.215032   \n",
       "\n",
       "   lG_Ins_lg_and_S_cent_ins  rG_Ins_lg_and_S_cent_ins  lG_insular_short  \\\n",
       "0                  3.281339                  3.034438          3.676079   \n",
       "1                  3.100075                  2.620097          3.565704   \n",
       "2                  3.579311                  3.203484          4.116234   \n",
       "3                  3.077230                  2.646450          3.761522   \n",
       "4                  3.089306                  2.673509          3.275040   \n",
       "\n",
       "   rG_insular_short  lG_occipital_middle  rG_occipital_middle  \\\n",
       "0          3.898588             2.257806             2.326474   \n",
       "1          3.082581             2.508954             2.523208   \n",
       "2          3.956138             2.340168             2.404881   \n",
       "3          3.982027             2.428474             2.336983   \n",
       "4          3.970294             2.479741             2.431082   \n",
       "\n",
       "   lG_occipital_sup  rG_occipital_sup  lG_oc-temp_lat-fusifor  \\\n",
       "0          2.082201          1.964689                2.331578   \n",
       "1          1.954296          1.911302                2.700320   \n",
       "2          2.062779          2.139359                2.735869   \n",
       "3          1.956718          1.967924                2.730821   \n",
       "4          1.927752          1.967944                2.320989   \n",
       "\n",
       "   rG_oc-temp_lat-fusifor  lG_oc-temp_med-Lingual  rG_oc-temp_med-Lingual  \\\n",
       "0                2.490000                1.842906                1.933253   \n",
       "1                2.606456                1.628881                1.708448   \n",
       "2                2.530499                1.799470                1.901053   \n",
       "3                2.639524                1.859430                1.907578   \n",
       "4                2.565484                1.750776                1.680309   \n",
       "\n",
       "   lG_oc-temp_med-Parahip  rG_oc-temp_med-Parahip  lG_orbital  rG_orbital  \\\n",
       "0                2.921091                3.126393    2.695209    2.805978   \n",
       "1                2.466486                2.546927    2.686398    2.659456   \n",
       "2                3.054599                3.155449    2.519554    2.654329   \n",
       "3                2.460135                2.943835    2.386702    2.399509   \n",
       "4                2.337275                2.783960    2.771243    2.918862   \n",
       "\n",
       "   lG_pariet_inf-Angular  rG_pariet_inf-Angular  lG_pariet_inf-Supramar  \\\n",
       "0               2.422602               2.501791                2.411938   \n",
       "1               2.553232               2.422354                2.465979   \n",
       "2               2.445448               2.538123                2.313413   \n",
       "3               2.321229               2.484544                2.328901   \n",
       "4               2.198639               2.325711                2.363832   \n",
       "\n",
       "   rG_pariet_inf-Supramar  lG_parietal_sup  rG_parietal_sup  lG_postcentral  \\\n",
       "0                2.372590         2.196704         2.141227        1.978632   \n",
       "1                2.571069         2.365390         2.202038        1.711097   \n",
       "2                2.547944         2.200662         2.221020        1.833015   \n",
       "3                2.391663         2.389039         2.290712        2.033650   \n",
       "4                2.341019         2.012848         2.066757        1.803862   \n",
       "\n",
       "   rG_postcentral  lG_precentral  rG_precentral  lG_precuneus  rG_precuneus  \\\n",
       "0        2.078509       1.924300       2.039493      2.372580      2.374773   \n",
       "1        1.457784       2.171960       2.194586      2.333405      2.318397   \n",
       "2        1.793619       2.074219       2.109261      2.279788      2.402283   \n",
       "3        1.903333       2.356269       2.489546      2.367732      2.409797   \n",
       "4        1.614757       1.902237       1.848689      2.205156      2.253479   \n",
       "\n",
       "   lG_rectus  rG_rectus  lG_subcallosal  rG_subcallosal  \\\n",
       "0   2.291711   2.495232        2.543986        3.220273   \n",
       "1   2.309580   2.220330        3.316470        2.589482   \n",
       "2   2.401778   2.445999        3.251933        2.980795   \n",
       "3   2.210943   2.120643        2.858720        2.474946   \n",
       "4   2.235300   2.174727        2.433027        3.080570   \n",
       "\n",
       "   lG_temp_sup-G_T_transv  rG_temp_sup-G_T_transv  lG_temp_sup-Lateral  \\\n",
       "0                1.908910                2.289189             2.722966   \n",
       "1                2.443934                2.436524             2.769449   \n",
       "2                2.090480                2.134567             2.621562   \n",
       "3                1.815059                1.743792             2.733618   \n",
       "4                1.752716                1.712989             2.410415   \n",
       "\n",
       "   rG_temp_sup-Lateral  lG_temp_sup-Plan_polar  rG_temp_sup-Plan_polar  \\\n",
       "0             2.812194                3.404710                3.642868   \n",
       "1             2.912509                2.966688                2.610880   \n",
       "2             2.783181                3.257760                3.462884   \n",
       "3             2.774414                2.809290                3.254334   \n",
       "4             2.733909                2.722359                3.074588   \n",
       "\n",
       "   lG_temp_sup-Plan_tempo  rG_temp_sup-Plan_tempo  lG_temporal_inf  \\\n",
       "0                2.352714                2.375830         2.681285   \n",
       "1                2.444941                2.479290         2.640138   \n",
       "2                2.105893                2.402274         2.605612   \n",
       "3                2.446254                2.135171         2.704359   \n",
       "4                2.343121                2.116158         2.363601   \n",
       "\n",
       "   rG_temporal_inf  lG_temporal_middle  rG_temporal_middle  \\\n",
       "0         2.757351            2.710977            2.849796   \n",
       "1         2.710535            2.662709            2.805806   \n",
       "2         2.724318            2.650980            2.749851   \n",
       "3         2.777295            2.444769            2.612276   \n",
       "4         2.644726            2.457741            2.609281   \n",
       "\n",
       "   lLat_Fis-ant-Horizont  rLat_Fis-ant-Horizont  lLat_Fis-ant-Vertical  \\\n",
       "0               2.299123               2.367012               2.341449   \n",
       "1               2.382342               2.431699               2.346092   \n",
       "2               2.507766               2.280558               2.498169   \n",
       "3               2.063223               2.308239               2.130290   \n",
       "4               2.296422               2.282857               2.078151   \n",
       "\n",
       "   rLat_Fis-ant-Vertical  lLat_Fis-post  rLat_Fis-post  lMedial_wall  \\\n",
       "0               2.343801       1.968207       2.403490           NaN   \n",
       "1               2.546148       2.255039       2.351424           NaN   \n",
       "2               2.433188       2.162759       2.480639           NaN   \n",
       "3               1.829609       1.979185       2.210194           NaN   \n",
       "4               2.624582       2.244507       2.350743           NaN   \n",
       "\n",
       "   rMedial_wall  lPole_occipital  rPole_occipital  lPole_temporal  \\\n",
       "0           NaN         1.831361         1.925835        3.106892   \n",
       "1           NaN         1.796661         1.757166        3.112608   \n",
       "2           NaN         1.892133         1.950294        3.021313   \n",
       "3           NaN         1.960188         1.950527        3.017652   \n",
       "4           NaN         1.879614         2.051437        2.637061   \n",
       "\n",
       "   rPole_temporal  lS_calcarine  rS_calcarine  lS_central  rS_central  \\\n",
       "0        3.173338      1.787686      2.043565    1.595498    1.661827   \n",
       "1        2.920239      1.595840      1.686719    1.347035    1.287529   \n",
       "2        3.078840      1.692471      1.744289    1.518490    1.487509   \n",
       "3        3.085552      1.739287      1.918926    1.748103    1.674920   \n",
       "4        2.989546      1.823235      1.760244    1.390253    1.339797   \n",
       "\n",
       "   lS_cingul-Marginalis  rS_cingul-Marginalis  lS_circular_insula_ant  \\\n",
       "0              2.404563              2.308197                3.458154   \n",
       "1              2.302328              2.054686                3.146097   \n",
       "2              2.065293              2.098563                3.169226   \n",
       "3              2.362947              2.336657                2.664091   \n",
       "4              1.922822              2.001445                3.226001   \n",
       "\n",
       "   rS_circular_insula_ant  lS_circular_insula_inf  rS_circular_insula_inf  \\\n",
       "0                3.415487                3.000959                3.066050   \n",
       "1                2.909812                2.743656                2.612447   \n",
       "2                3.262496                2.921727                2.701621   \n",
       "3                2.780160                2.393347                2.543878   \n",
       "4                3.236086                2.502783                2.582805   \n",
       "\n",
       "   lS_circular_insula_sup  rS_circular_insula_sup  lS_collat_transv_ant  \\\n",
       "0                2.725759                2.919006              2.802477   \n",
       "1                2.633974                2.751714              2.569960   \n",
       "2                2.843148                3.028769              2.516392   \n",
       "3                2.382239                2.685439              2.508114   \n",
       "4                2.682365                2.965817              2.232335   \n",
       "\n",
       "   rS_collat_transv_ant  lS_collat_transv_post  rS_collat_transv_post  \\\n",
       "0              2.654570               1.822014               1.979481   \n",
       "1              2.604781               2.022331               2.178153   \n",
       "2              2.509358               1.986084               1.794813   \n",
       "3              2.734307               1.976895               1.968817   \n",
       "4              2.365675               1.862728               2.039689   \n",
       "\n",
       "   lS_front_inf  rS_front_inf  lS_front_middle  rS_front_middle  lS_front_sup  \\\n",
       "0      2.138143      2.206768         2.186763         2.219792      2.522033   \n",
       "1      2.318094      2.396334         2.272284         2.327795      2.473423   \n",
       "2      2.166686      2.148551         2.151709         2.130060      2.317211   \n",
       "3      2.132622      2.093264         2.098069         2.047001      2.459728   \n",
       "4      2.167444      2.209746         2.176439         2.120962      2.161039   \n",
       "\n",
       "   rS_front_sup  lS_interm_prim-Jensen  rS_interm_prim-Jensen  \\\n",
       "0      2.359838               2.301323               2.402576   \n",
       "1      2.671409               2.279328               2.424174   \n",
       "2      2.245349               2.096711               2.178812   \n",
       "3      2.404702               2.053868               2.331705   \n",
       "4      2.224672               2.127076               1.985744   \n",
       "\n",
       "   lS_intrapariet_and_P_trans  rS_intrapariet_and_P_trans  \\\n",
       "0                    2.041849                    2.140886   \n",
       "1                    2.153820                    2.185688   \n",
       "2                    2.115657                    2.154094   \n",
       "3                    2.156128                    2.186819   \n",
       "4                    1.929475                    1.891974   \n",
       "\n",
       "   lS_oc_middle_and_Lunatus  rS_oc_middle_and_Lunatus  \\\n",
       "0                  2.015811                  1.957807   \n",
       "1                  2.130158                  2.180025   \n",
       "2                  2.114987                  2.057267   \n",
       "3                  2.240412                  2.140361   \n",
       "4                  2.057004                  2.197119   \n",
       "\n",
       "   lS_oc_sup_and_transversal  rS_oc_sup_and_transversal  lS_occipital_ant  \\\n",
       "0                   2.070450                   1.968242          2.163826   \n",
       "1                   2.128435                   2.119946          2.033292   \n",
       "2                   2.020059                   2.015122          2.285404   \n",
       "3                   2.273446                   2.033141          2.026850   \n",
       "4                   2.084604                   2.078425          2.291279   \n",
       "\n",
       "   rS_occipital_ant  lS_oc-temp_lat  rS_oc-temp_lat  \\\n",
       "0          2.058096        2.493632        2.539819   \n",
       "1          2.112388        2.331296        2.222434   \n",
       "2          2.139183        2.556137        2.513475   \n",
       "3          1.996814        2.479676        2.444856   \n",
       "4          2.269948        2.260950        2.479011   \n",
       "\n",
       "   lS_oc-temp_med_and_Lingual  rS_oc-temp_med_and_Lingual  lS_orbital_lateral  \\\n",
       "0                    2.240415                    2.370086            2.217716   \n",
       "1                    2.181254                    2.099374            2.178623   \n",
       "2                    2.365632                    2.628261            2.409998   \n",
       "3                    2.286056                    2.220625            1.950955   \n",
       "4                    2.259284                    2.420669            2.210238   \n",
       "\n",
       "   rS_orbital_lateral  lS_orbital_med-olfact  rS_orbital_med-olfact  \\\n",
       "0            2.062718               2.266284               2.568335   \n",
       "1            2.452816               2.211108               2.258118   \n",
       "2            2.113767               2.408826               2.536114   \n",
       "3            1.951106               2.183467               2.160477   \n",
       "4            2.341858               2.241286               2.376118   \n",
       "\n",
       "   lS_orbital-H_Shaped  rS_orbital-H_Shaped  lS_parieto_occipital  \\\n",
       "0             2.498707             2.477320              2.001671   \n",
       "1             2.417704             2.541279              1.965445   \n",
       "2             2.489287             2.528121              2.020478   \n",
       "3             2.562018             2.718355              1.927688   \n",
       "4             2.529711             2.588473              2.038059   \n",
       "\n",
       "   rS_parieto_occipital  lS_pericallosal  rS_pericallosal  lS_postcentral  \\\n",
       "0              2.098518         1.807530         1.877744        2.094920   \n",
       "1              2.077335         1.817071         1.947190        2.128129   \n",
       "2              2.130425         1.800021         1.567593        1.927498   \n",
       "3              1.931133         1.605575         1.804221        2.043672   \n",
       "4              2.150270         1.679060         1.503930        1.808304   \n",
       "\n",
       "   rS_postcentral  lS_precentral-inf-part  rS_precentral-inf-part  \\\n",
       "0        2.176244                2.394351                2.386774   \n",
       "1        1.878907                2.318670                2.412602   \n",
       "2        1.941317                2.427749                2.326529   \n",
       "3        2.148453                2.238210                2.145806   \n",
       "4        1.784406                2.192276                2.057184   \n",
       "\n",
       "   lS_precentral-sup-part  rS_precentral-sup-part  lS_suborbital  \\\n",
       "0                2.119276                2.155038       2.234450   \n",
       "1                2.159158                2.310238       2.445196   \n",
       "2                2.153913                2.119882       2.608098   \n",
       "3                2.440251                2.351300       2.126382   \n",
       "4                2.167805                1.942588       2.241874   \n",
       "\n",
       "   rS_suborbital  lS_subparietal  rS_subparietal  lS_temporal_inf  \\\n",
       "0       2.244920        2.282227        2.256248         2.373447   \n",
       "1       2.248049        2.333428        2.327283         2.484239   \n",
       "2       2.283140        2.366019        2.215158         2.330570   \n",
       "3       2.036968        2.238736        2.386632         2.442251   \n",
       "4       2.002333        2.274040        2.126398         2.140593   \n",
       "\n",
       "   rS_temporal_inf  lS_temporal_sup  rS_temporal_sup  lS_temporal_transverse  \\\n",
       "0         2.439594         2.442846         2.418286                1.946976   \n",
       "1         2.456764         2.430170         2.508620                2.172205   \n",
       "2         2.437196         2.386827         2.519870                1.931591   \n",
       "3         2.331181         2.252620         2.375511                1.757637   \n",
       "4         2.405042         2.156671         2.309237                2.050010   \n",
       "\n",
       "   rS_temporal_transverse  \n",
       "0                1.929366  \n",
       "1                2.547939  \n",
       "2                2.130694  \n",
       "3                1.856733  \n",
       "4                1.831899  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1744972538843,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "1f16ba61-ddf7-4767-8fa0-514d21d8b5f2",
    "outputId": "8afdb7e9-0067-4ab4-cb82-c53aaa05160d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1074, 154)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1744972538977,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "42aeb85d-80ec-4594-a8d5-8e1668d69073",
    "outputId": "991bac37-1c8f-42e1-8859-b286e8bb2cd4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lUnknown                         0\n",
       "rUnknown                         0\n",
       "lG_and_S_frontomargin            0\n",
       "rG_and_S_frontomargin            0\n",
       "lG_and_S_occipital_inf           0\n",
       "rG_and_S_occipital_inf           0\n",
       "lG_and_S_paracentral             0\n",
       "rG_and_S_paracentral             0\n",
       "lG_and_S_subcentral              0\n",
       "rG_and_S_subcentral              0\n",
       "lG_and_S_transv_frontopol        0\n",
       "rG_and_S_transv_frontopol        0\n",
       "lG_and_S_cingul-Ant              0\n",
       "rG_and_S_cingul-Ant              0\n",
       "lG_and_S_cingul-Mid-Ant          0\n",
       "rG_and_S_cingul-Mid-Ant          0\n",
       "lG_and_S_cingul-Mid-Post         0\n",
       "rG_and_S_cingul-Mid-Post         0\n",
       "lG_cingul-Post-dorsal            0\n",
       "rG_cingul-Post-dorsal            0\n",
       "lG_cingul-Post-ventral           0\n",
       "rG_cingul-Post-ventral           0\n",
       "lG_cuneus                        0\n",
       "rG_cuneus                        0\n",
       "lG_front_inf-Opercular           0\n",
       "rG_front_inf-Opercular           0\n",
       "lG_front_inf-Orbital             0\n",
       "rG_front_inf-Orbital             0\n",
       "lG_front_inf-Triangul            0\n",
       "rG_front_inf-Triangul            0\n",
       "lG_front_middle                  0\n",
       "rG_front_middle                  0\n",
       "lG_front_sup                     0\n",
       "rG_front_sup                     0\n",
       "lG_Ins_lg_and_S_cent_ins         0\n",
       "rG_Ins_lg_and_S_cent_ins         0\n",
       "lG_insular_short                 0\n",
       "rG_insular_short                 0\n",
       "lG_occipital_middle              0\n",
       "rG_occipital_middle              0\n",
       "lG_occipital_sup                 0\n",
       "rG_occipital_sup                 0\n",
       "lG_oc-temp_lat-fusifor           0\n",
       "rG_oc-temp_lat-fusifor           0\n",
       "lG_oc-temp_med-Lingual           0\n",
       "rG_oc-temp_med-Lingual           0\n",
       "lG_oc-temp_med-Parahip           0\n",
       "rG_oc-temp_med-Parahip           0\n",
       "lG_orbital                       0\n",
       "rG_orbital                       0\n",
       "lG_pariet_inf-Angular            0\n",
       "rG_pariet_inf-Angular            0\n",
       "lG_pariet_inf-Supramar           0\n",
       "rG_pariet_inf-Supramar           0\n",
       "lG_parietal_sup                  0\n",
       "rG_parietal_sup                  0\n",
       "lG_postcentral                   0\n",
       "rG_postcentral                   0\n",
       "lG_precentral                    0\n",
       "rG_precentral                    0\n",
       "lG_precuneus                     0\n",
       "rG_precuneus                     0\n",
       "lG_rectus                        0\n",
       "rG_rectus                        0\n",
       "lG_subcallosal                   0\n",
       "rG_subcallosal                   0\n",
       "lG_temp_sup-G_T_transv           0\n",
       "rG_temp_sup-G_T_transv           0\n",
       "lG_temp_sup-Lateral              0\n",
       "rG_temp_sup-Lateral              0\n",
       "lG_temp_sup-Plan_polar           0\n",
       "rG_temp_sup-Plan_polar           0\n",
       "lG_temp_sup-Plan_tempo           0\n",
       "rG_temp_sup-Plan_tempo           0\n",
       "lG_temporal_inf                  0\n",
       "rG_temporal_inf                  0\n",
       "lG_temporal_middle               0\n",
       "rG_temporal_middle               0\n",
       "lLat_Fis-ant-Horizont            0\n",
       "rLat_Fis-ant-Horizont            0\n",
       "lLat_Fis-ant-Vertical            0\n",
       "rLat_Fis-ant-Vertical            0\n",
       "lLat_Fis-post                    0\n",
       "rLat_Fis-post                    0\n",
       "lMedial_wall                  1074\n",
       "rMedial_wall                  1074\n",
       "lPole_occipital                  0\n",
       "rPole_occipital                  0\n",
       "lPole_temporal                   0\n",
       "rPole_temporal                   0\n",
       "lS_calcarine                     0\n",
       "rS_calcarine                     0\n",
       "lS_central                       0\n",
       "rS_central                       0\n",
       "lS_cingul-Marginalis             0\n",
       "rS_cingul-Marginalis             0\n",
       "lS_circular_insula_ant           0\n",
       "rS_circular_insula_ant           0\n",
       "lS_circular_insula_inf           0\n",
       "rS_circular_insula_inf           0\n",
       "lS_circular_insula_sup           0\n",
       "rS_circular_insula_sup           0\n",
       "lS_collat_transv_ant             0\n",
       "rS_collat_transv_ant             0\n",
       "lS_collat_transv_post            0\n",
       "rS_collat_transv_post            0\n",
       "lS_front_inf                     0\n",
       "rS_front_inf                     0\n",
       "lS_front_middle                  0\n",
       "rS_front_middle                  0\n",
       "lS_front_sup                     0\n",
       "rS_front_sup                     0\n",
       "lS_interm_prim-Jensen            0\n",
       "rS_interm_prim-Jensen            0\n",
       "lS_intrapariet_and_P_trans       0\n",
       "rS_intrapariet_and_P_trans       0\n",
       "lS_oc_middle_and_Lunatus         0\n",
       "rS_oc_middle_and_Lunatus         0\n",
       "lS_oc_sup_and_transversal        0\n",
       "rS_oc_sup_and_transversal        0\n",
       "lS_occipital_ant                 0\n",
       "rS_occipital_ant                 0\n",
       "lS_oc-temp_lat                   0\n",
       "rS_oc-temp_lat                   0\n",
       "lS_oc-temp_med_and_Lingual       0\n",
       "rS_oc-temp_med_and_Lingual       0\n",
       "lS_orbital_lateral               0\n",
       "rS_orbital_lateral               0\n",
       "lS_orbital_med-olfact            0\n",
       "rS_orbital_med-olfact            0\n",
       "lS_orbital-H_Shaped              0\n",
       "rS_orbital-H_Shaped              0\n",
       "lS_parieto_occipital             0\n",
       "rS_parieto_occipital             0\n",
       "lS_pericallosal                  0\n",
       "rS_pericallosal                  0\n",
       "lS_postcentral                   0\n",
       "rS_postcentral                   0\n",
       "lS_precentral-inf-part           0\n",
       "rS_precentral-inf-part           0\n",
       "lS_precentral-sup-part           0\n",
       "rS_precentral-sup-part           0\n",
       "lS_suborbital                    0\n",
       "rS_suborbital                    0\n",
       "lS_subparietal                   0\n",
       "rS_subparietal                   0\n",
       "lS_temporal_inf                  0\n",
       "rS_temporal_inf                  0\n",
       "lS_temporal_sup                  0\n",
       "rS_temporal_sup                  0\n",
       "lS_temporal_transverse           0\n",
       "rS_temporal_transverse           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "y_classification = data['DX_bl']\n",
    "X = data.drop(columns=['RID', 'DX_bl'])\n",
    "\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744972538981,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "3d27311a-b773-4c68-859e-ab1a1f9ea1a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.drop(columns= ['lMedial_wall', 'rMedial_wall'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZq27fGnlDTX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z5QEU_AlDzY"
   },
   "source": [
    "### Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2238,
     "status": "ok",
     "timestamp": 1744972541221,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "MBbWhGLvlGaE",
    "outputId": "3597ac84-4401-4f90-bd46-6254c56a7029"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "corr = X.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "threshold = 0.7\n",
    "filtered_corr = corr[((corr >= threshold) | (corr <= -threshold)) & (corr != 1.0)]\n",
    "\n",
    "sns.clustermap(corr, cmap=\"coolwarm\", figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1744972542020,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "wemT5vcylXGr",
    "outputId": "e612ce0f-113d-443f-8c46-e2310be27779"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 16))  # adjust size depending on how many columns\n",
    "sns.heatmap(filtered_corr, mask=mask, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTFluvQilvi5"
   },
   "source": [
    "### Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744972542025,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "f1c59ff4-2102-49e4-b0eb-357486afd4a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from cuml.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = X.astype('float32')\n",
    "y_clas_enc = LabelEncoder()\n",
    "y_classification = y_clas_enc.fit_transform(y_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1744972542207,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "0c9abc8e-3ff0-4c84-9df7-da9ef2ac431d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from cuml.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_classification, test_size=0.2, random_state=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kY3wd1WUZbds"
   },
   "source": [
    "## Problem (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Suh9TxKraQ_l"
   },
   "source": [
    "### Estimate a RF to have a general idea of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1306,
     "status": "ok",
     "timestamp": 1744972543514,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "8i3cWLXTB0XI",
    "outputId": "fa2d09d9-299b-4b26-add8-50eb520f7f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc is 0.6465116279069767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "model = RandomForestClassifier(random_state=seed)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Val acc is {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTsgJc0HaXaC"
   },
   "source": [
    "### RF Optimization Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1744972989494,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "RBHW9zjsAa7X"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# VERSION WITH STATIFIED K FOLD CV and\n",
    "\n",
    "# Define the optimization problem\n",
    "class RFOptimizationProblem:\n",
    "    def __init__(self, bounds, X_train, X_test, y_train, y_test):\n",
    "        self.bounds = bounds  # Define search space\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.dim = bounds.shape[1]\n",
    "        self.num_objectives = bounds.shape[0] #2 for us always\n",
    "\n",
    "        max_depth_bound = self.bounds[1,1]\n",
    "\n",
    "        self.ref_point = torch.tensor([0, -1 * max_depth_bound + 2]).to(**tkwargs)  # Reference point for hypervolume, assume that it is the lower bound of the objective functions\n",
    "\n",
    "\n",
    "    def evaluate_rf(self, x):\n",
    "        \"\"\"\n",
    "        Trains an RF model for a row x and returns accuracy and tree depth.\n",
    "        x: Tensor containing a set of hyperparameters, shape (1, 4)\n",
    "\n",
    "        Returns:\n",
    "            - Accuracy (to maximize)\n",
    "            - Depth (to minimize)\n",
    "        \"\"\"\n",
    "\n",
    "        n_estimators = int(x[0].item())\n",
    "        max_depth = int(x[1].item()) if x[1].item() > 0 else None\n",
    "        min_samples_split = int(x[2].item())\n",
    "        min_samples_leaf = int(x[3].item())\n",
    "\n",
    "        # Train Random Forest\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=seed\n",
    "        )\n",
    "        #model.fit(self.X_train, self.y_train)\n",
    "        # If we do cv we dont want to fit, we need to fit with the cross val score\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "        # Perform cross-validation with the defined splitter\n",
    "        cv_acc = cross_val_score(model, self.X_train, self.y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "        accuracy = np.mean(cv_acc)\n",
    "        # TODO: We could add a acc that is mean - std to express the std too\n",
    "\n",
    "        results = [accuracy, max_depth]\n",
    "\n",
    "        return torch.tensor(results, **tkwargs)  # Return tensor with shape (batch_size, 2)\n",
    "\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        \"\"\"\n",
    "        Given input hyperparameters `x`, evaluate the RF model.\n",
    "        Returns [accuracy, depth] (accuracy to be maximized, depth to be minimized).\n",
    "        \"\"\"\n",
    "        results = self.evaluate_rf(x)  \n",
    "        results[1] *= -1\n",
    "        return results # Negate depth for minimization\n",
    "\n",
    "\n",
    "\n",
    "bounds = torch.tensor([\n",
    "    [10.0, 2.0, 2.0, 1.0],   # Lower bounds: n_estimators, max_depth, min_samples_split, min_samples_leaf\n",
    "    [200.0, 100.0, 10.0, 10.0] # Upper bounds\n",
    "], **tkwargs)  # Shape (4,2) for Botorch\n",
    "\n",
    "# Instantiate problem\n",
    "problem = RFOptimizationProblem(bounds, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21791,
     "status": "ok",
     "timestamp": 1744973021911,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "Z5dOXg4fnzPr",
    "outputId": "9bf79d47-b3c2-472d-a569-8411374fd8e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[199.5275,  12.2279,   8.5838,   4.7749],\n",
      "        [ 96.0132,  84.1306,   2.5862,   6.5455],\n",
      "        [ 51.4549,  36.3218,   7.5834,   2.5657],\n",
      "        [149.0785,  59.2611,   5.5865,   8.6789],\n",
      "        [121.3842,  40.2496,   3.2393,   9.1866],\n",
      "        [ 29.7909,  69.3130,   9.2432,   2.0633]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor([  0.7218, -12.0000], device='cuda:0', dtype=torch.float64)\n",
      "tensor([  0.7194, -84.0000], device='cuda:0', dtype=torch.float64)\n",
      "tensor([  0.7252, -36.0000], device='cuda:0', dtype=torch.float64)\n",
      "tensor([  0.7229, -59.0000], device='cuda:0', dtype=torch.float64)\n",
      "tensor([  0.7276, -40.0000], device='cuda:0', dtype=torch.float64)\n",
      "tensor([  0.7252, -69.0000], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data = draw_sobol_samples(bounds=problem.bounds, n=6, q=1).squeeze(1)\n",
    "print(data)\n",
    "for x in data:\n",
    "  print(problem.evaluate(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU-ytuKybL5a"
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1744973021926,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "at0mcEXGZksW"
   },
   "outputs": [],
   "source": [
    "def generate_initial_data(n=6):\n",
    "    # Generate training data\n",
    "    train_x = draw_sobol_samples(bounds=problem.bounds, n=n, q=1).squeeze(1)\n",
    "\n",
    "    #Evaluate the objective function\n",
    "    train_obj_true = torch.stack([problem.evaluate(x) for x in train_x])\n",
    "\n",
    "    # TODO: check con profe que esto esta bien aunque creo que sí: Add noise to simulate real-world variation\n",
    "    #train_obj = train_obj_true + torch.randn_like(train_obj_true)\n",
    "\n",
    "    return train_x, train_obj_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25122,
     "status": "ok",
     "timestamp": 1744973047049,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "ZMntAjtcPHGR",
    "outputId": "faa76b3f-8be8-4be1-fac0-7d0713a22242"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[141.0982,  55.5528,   7.9934,   7.0434],\n",
       "         [ 63.3311,  28.9021,   2.9933,   2.6879],\n",
       "         [ 20.5308,  78.0080,   9.6791,   8.4094],\n",
       "         [195.0139,   6.2825,   4.6788,   3.5715],\n",
       "         [175.6634,  97.2847,   3.9465,   4.9322],\n",
       "         [ 39.9132,  22.4009,   6.9466,   9.5866]], device='cuda:0',\n",
       "        dtype=torch.float64),\n",
       " tensor([[  0.7194, -55.0000],\n",
       "         [  0.7299, -28.0000],\n",
       "         [  0.7054, -78.0000],\n",
       "         [  0.7241,  -6.0000],\n",
       "         [  0.7241, -97.0000],\n",
       "         [  0.7287, -22.0000]], device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_initial_data(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744973047050,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "InHcy1lb98gq"
   },
   "outputs": [],
   "source": [
    "def initialize_model(train_x, train_obj_true):\n",
    "    # define models for objective and constraint\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "    models = []\n",
    "    for i in range(train_obj_true.shape[-1]):\n",
    "        train_y = train_obj_true[..., i : i + 1]\n",
    "        #train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
    "        models.append(\n",
    "            SingleTaskGP(\n",
    "                train_x, train_y, outcome_transform=Standardize(m=1)\n",
    "            )\n",
    "        )\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV-ozgfBbP5r"
   },
   "source": [
    "# Opti QEHVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXJgO2HLglfN"
   },
   "source": [
    "From tutorial:\n",
    "\n",
    "qEHVI requires specifying a reference point, which is the lower bound on the objectives used for computing hypervolume. In this tutorial, we assume the reference point is known. In practice the reference point can be set 1) using domain knowledge to be slightly worse than the lower bound of objective values, where the lower bound is the minimum acceptable value of interest for each objective, or 2) using a dynamic reference point selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1744973047052,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "0v5RDIZyeMng"
   },
   "outputs": [],
   "source": [
    "SMOKE_TEST = True\n",
    "BATCH_SIZE = 2\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 if not SMOKE_TEST else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1744973047053,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "P-3KIHfz-W6P"
   },
   "outputs": [],
   "source": [
    "from botorch.acquisition.multi_objective.logei import qLogExpectedHypervolumeImprovement\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "\n",
    "    normalized_bounds = torch.stack([\n",
    "                torch.zeros_like(problem.bounds[0]),\n",
    "                torch.ones_like(problem.bounds[1]),\n",
    "                ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, problem.bounds)).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=problem.ref_point,\n",
    "        Y=pred,\n",
    "    )\n",
    "    acq_func = qLogExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=normalized_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values\n",
    "    new_x = unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = torch.stack([problem.evaluate(x) for x in new_x])\n",
    "    #new_noise = new_x[:, -2:]\n",
    "\n",
    "    #new_obj = new_obj_true + torch.randn_like(new_obj_true) * new_noise\n",
    "    return new_x, new_obj_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLIgKTj_brIk"
   },
   "source": [
    "# Opti QNEHVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1744973047055,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "KNSraiX2_g_O"
   },
   "outputs": [],
   "source": [
    "from botorch.acquisition.multi_objective.logei import qLogNoisyExpectedHypervolumeImprovement\n",
    "\n",
    "\n",
    "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    normalized_bounds = torch.stack([\n",
    "                torch.zeros_like(problem.bounds[0]),\n",
    "                torch.ones_like(problem.bounds[1]),\n",
    "                ])\n",
    "\n",
    "    acq_func = qLogNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point.tolist(),  # use known reference point\n",
    "        X_baseline=normalize(train_x, problem.bounds),\n",
    "        prune_baseline=True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=normalized_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values\n",
    "    new_x = unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = torch.stack([problem.evaluate(x) for x in new_x])\n",
    "    #new_noise = new_x[:, -2:]\n",
    "\n",
    "    #new_obj = new_obj_true + torch.randn_like(new_obj_true) * new_noise\n",
    "    return new_x, new_obj_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2__lmiibvl4"
   },
   "source": [
    "# Opti QNParego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1744973047056,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "SaXMUT3u_obp"
   },
   "outputs": [],
   "source": [
    "from botorch.acquisition.logei import qLogNoisyExpectedImprovement\n",
    "\n",
    "\n",
    "def optimize_qnparego_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Samples a set of random weights for each candidate in the batch, performs sequential greedy optimization\n",
    "    of the qNParEGO acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "\n",
    "    normalized_bounds = torch.stack([\n",
    "                torch.zeros_like(problem.bounds[0]),\n",
    "                torch.ones_like(problem.bounds[1]),\n",
    "                ])\n",
    "\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(train_x).mean\n",
    "    acq_func_list = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        weights = sample_simplex(problem.num_objectives, **tkwargs).squeeze()\n",
    "        objective = GenericMCObjective(\n",
    "            get_chebyshev_scalarization(weights=weights, Y=pred)\n",
    "        )\n",
    "        acq_func = qLogNoisyExpectedImprovement(  # pyre-ignore: [28]\n",
    "            model=model,\n",
    "            objective=objective,\n",
    "            X_baseline=train_x,\n",
    "            sampler=sampler,\n",
    "            prune_baseline=True,\n",
    "        )\n",
    "        acq_func_list.append(acq_func)\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf_list(\n",
    "        acq_function_list=acq_func_list,\n",
    "        bounds=normalized_bounds,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    )\n",
    "    # observe new values\n",
    "\n",
    "    new_x = unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = torch.stack([problem.evaluate(x) for x in new_x])\n",
    "    #new_noise = new_x[:, -2:]\n",
    "\n",
    "    #new_obj = new_obj_true + torch.randn_like(new_obj_true) * new_noise\n",
    "    return new_x, new_obj_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "incUe6hVb2iG"
   },
   "source": [
    "# Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744973854813,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "qklyLN15iyib"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import DominatedPartitioning\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# set the run id if we want to run each exp more than once\n",
    "run_number = 0\n",
    "\n",
    "N_BATCH = 50 if not SMOKE_TEST else 2\n",
    "MC_SAMPLES = 128 if not SMOKE_TEST else 16\n",
    "verbose = True\n",
    "\n",
    "\n",
    "# def should_continue(hvs_list, current_volume, tol_window=3, tolerance = 0.0001):\n",
    "#     tol_window += 1\n",
    "#     if len(hvs_list) < tol_window:\n",
    "#         return True\n",
    "#     prev_mean = sum(hvs_list[-tol_window:-1]) / tol_window\n",
    "#     return abs(current_volume - prev_mean) < tolerance\n",
    "\n",
    "def check_hv_early_stopping(hvs_list, threshold=1e-2, patience=5):\n",
    "    \"\"\"\n",
    "    Early stopping if HV has not improved more than `threshold`\n",
    "    for `patience` consecutive iterations.\n",
    "\n",
    "    Parameters:\n",
    "    - hvs_list: list of HV values per iteration\n",
    "    - threshold: min improvement to be considered \"progress\"\n",
    "    - patience: number of allowed consecutive stagnant iterations\n",
    "    \"\"\"\n",
    "    if len(hvs_list) < patience:\n",
    "        return True\n",
    "\n",
    "    # Count how many iterations since last significant improvement\n",
    "    stagnant_iters = 0\n",
    "    for i in range(len(hvs_list) - 1, 0, -1):\n",
    "        delta = hvs_list[i] - hvs_list[i - 1]\n",
    "        if delta < threshold:\n",
    "            stagnant_iters += 1\n",
    "        else:\n",
    "            break  # reset count on any significant improvement\n",
    "\n",
    "        if stagnant_iters >= patience:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def plot_hv_progress(hv_dict):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for label, values in hv_dict.items():\n",
    "        plt.plot(values, label=label)\n",
    "\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Hypervolume\")\n",
    "    plt.title(\"Hypervolume Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pareto_plot_2d(Y, filename=\"pareto_front_2d.png\"):\n",
    "    pareto_mask = is_non_dominated(Y)\n",
    "    pareto_Y = Y[pareto_mask]\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(Y[:, 0].numpy(), Y[:, 1].numpy(), label='All Points', alpha=0.3)\n",
    "    plt.scatter(pareto_Y[:, 0].numpy(), pareto_Y[:, 1].numpy(), color='red', label='Pareto Front')\n",
    "    plt.xlabel('Objective 1')\n",
    "    plt.ylabel('Objective 2')\n",
    "    plt.title(\"Final Pareto Front\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rows = []\n",
    "\n",
    "# Metadata for the current run\n",
    "metadata = {\n",
    "    \"dataset\": \"dataOK\",\n",
    "    \"gp_model\": \"SingleGP\",  # or \"MultiGP\"\n",
    "    \"acquisition\": \"qParEGO\",\n",
    "    \"seed\": seed,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"run_id\": run_number,  # if looping over multiple configurations\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 469053,
     "status": "ok",
     "timestamp": 1744974327046,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "WH9UDtmhizG8",
    "outputId": "579b5c52-3c56-4540-f867-73b599aae909"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAHqCAYAAADyGZa5AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZpZJREFUeJzt3Xd4FOX6//HPJmwaqUCAADGEXpUWkKKglKCigBxApIMcRJAmcASPAnpEsWAQC6A0jyIoTY8FBARUioSigCI99GICISSBtJ3fH/zYL0sCJGGTnSTv13Xl0nnm2dl79mbI3ky5LYZhGAIAAAAAAHnCzdUBAAAAAABQmFF4AwAAAACQhyi8AQAAAADIQxTeAAAAAADkIQpvAAAAAADyEIU3AAAAAAB5iMIbAAAAAIA8ROENAAAAAEAeovAGAAAAACAPUXgDAFCIzJ8/XxaLRTExMa4OBQAA/H8U3gCAAulagblt27Ys17dq1Up16tTJ56hwJyZNmiSLxWL/8fHxUa1atfTvf/9bCQkJrg4PAIBcK+bqAAAAAK734YcfytfXV4mJifrhhx/06quv6scff9TGjRtlsVhcHR4AADlG4Q0AgAskJSWpePHirg7DlP7xj3+oVKlSkqSnn35aXbp00bJly7RlyxY1bdo0y9ckJyfLx8cnX+IjdwCAnOJScwBAkdCyZUvdc889Wa6rXr26IiMjJUkxMTGyWCx666239M477ygsLEze3t5q2bKl9uzZk+m1f/31l/7xj3+oRIkS8vLyUqNGjfT11187zLl2WfyGDRv0zDPPqHTp0qpQoYKWLFliH7/RrFmzZLFYHN7zxx9/1H333afixYsrMDBQHTt21N69e2+77xaLRZMmTco0XrFiRfXr1y9TnL/88ouGDx+u4OBgBQYGavDgwUpNTVV8fLz69OmjoKAgBQUFady4cTIMw2GbNptNUVFRql27try8vFSmTBkNHjxYFy5cuG2cN/Pggw9Kko4cOSLp/24j2L59u+6//375+PhowoQJkqRz585p4MCBKlOmjLy8vHTPPfdowYIFmbYZFxen3r17y9/fX4GBgerbt69+//13WSwWzZ8/3z6vX79+8vX11aFDh/Twww/Lz89PPXv2zNG+btu2TZGRkSpVqpS8vb0VHh6uAQMGOMxZtGiRGjZsKD8/P/n7+6tu3bqaPn16rj8zAIC5cMYbAFCgXbx4UbGxsZnG09LSHJZ79+6tQYMGac+ePQ73fkdHR2v//v3697//7TD/k08+0aVLlzR06FBduXJF06dP14MPPqjdu3erTJkykqQ//vhDzZs3V/ny5fX888+rePHi+uKLL9SpUyctXbpUnTt3dtjmM888o+DgYL300ktKSkrSI488Il9fX33xxRdq2bKlw9zFixerdu3a9ljXrFmjhx56SJUqVdKkSZN0+fJlzZgxQ82bN9eOHTtUsWLFXH+GN3r22WdVtmxZTZ48WVu2bNHs2bMVGBioTZs26a677tKUKVP03Xff6c0331SdOnXUp08f+2sHDx6s+fPnq3///ho+fLiOHDmi9957Tzt37tTGjRtltVpzHM+hQ4ckSSVLlrSPxcXF6aGHHtITTzyhXr16qUyZMrp8+bJatWqlgwcPatiwYQoPD9eXX36pfv36KT4+XiNGjJB0tWB+9NFHtXXrVg0ZMkQ1atTQV199pb59+2b5/unp6YqMjFSLFi301ltv2c+sZ2dfz507p3bt2ik4OFjPP/+8AgMDFRMTo2XLltm3v3r1avXo0UOtW7fW1KlTJUl79+7Vxo0b7TEDAAo4AwCAAmjevHmGpFv+1K5d2z4/Pj7e8PLyMv71r385bGf48OFG8eLFjcTERMMwDOPIkSOGJMPb29s4ceKEfd6vv/5qSDJGjRplH2vdurVRt25d48qVK/Yxm81mNGvWzKhatWqmWFu0aGGkp6c7vH+PHj2M0qVLO4yfPn3acHNzM15++WX7WL169YzSpUsbcXFx9rHff//dcHNzM/r06ZPpvY4cOWIfk2RMnDgx02cYFhZm9O3bN9NrIyMjDZvNZh9v2rSpYbFYjKeffto+lp6eblSoUMFo2bKlfeznn382JBmfffaZw/usXLkyy/EbTZw40ZBk7Nu3z/j777+NI0eOGLNmzTI8PT2NMmXKGElJSYZhGEbLli0NScbMmTMdXh8VFWVIMj799FP7WGpqqtG0aVPD19fXSEhIMAzDMJYuXWpIMqKiouzzMjIyjAcffNCQZMybN88+3rdvX0OS8fzzzzu8V3b3dfny5YYkIzo6+qb7PWLECMPf3z/Tnw0AQOHBpeYAgALt/fff1+rVqzP93H333Q7zAgIC1LFjR33++ef2y6MzMjK0ePFiderUKdM9u506dVL58uXty40bN1aTJk303XffSZLOnz+vH3/8Ud26ddOlS5cUGxur2NhYxcXFKTIyUgcOHNDJkycdtjlo0CC5u7s7jHXv3l3nzp3T+vXr7WNLliyRzWZT9+7dJUmnT5/Wb7/9pn79+qlEiRL2eXfffbfatm1rj8lZBg4c6PAQsyZNmsgwDA0cONA+5u7urkaNGunw4cP2sS+//FIBAQFq27at/fOIjY1Vw4YN5evrq3Xr1mXr/atXr67g4GCFh4dr8ODBqlKlir799luHe7g9PT3Vv39/h9d99913Klu2rHr06GEfs1qtGj58uBITE+2X9K9cuVJWq1WDBg2yz3Nzc9PQoUNvGtOQIUMclrO7r4GBgZKkb775JtNVGNcEBgYqKSlJq1evzsanAwAoiLjUHABQoDVu3FiNGjXKNB4UFJTpEvQ+ffpo8eLF+vnnn3X//fdrzZo1Onv2rHr37p3p9VWrVs00Vq1aNX3xxReSpIMHD8owDL344ot68cUXs4zt3LlzDsV7eHh4pjnt27dXQECAFi9erNatW0u6epl5vXr1VK1aNUnS0aNHJV0tSG9Us2ZNrVq1yqkP/LrrrrsclgMCAiRJoaGhmcavv5/5wIEDunjxokqXLp3lds+dO5et91+6dKn8/f1ltVpVoUIFVa5cOdOc8uXLy8PDw2Hs6NGjqlq1qtzcHM8r1KxZ077+2n9DQkIyPYytSpUqWcZTrFgxVahQwWEsu/vasmVLdenSRZMnT9Y777yjVq1aqVOnTnryySfl6ekp6eotCF988YUeeughlS9fXu3atVO3bt3Uvn37LLcNACh4KLwBAEVGZGSkypQpo08//VT333+/Pv30U5UtW1Zt2rTJ8bZsNpskacyYMfYHs93oxkLO29s70xxPT0916tRJy5cv1wcffKCzZ89q48aNmjJlSo5jyqmMjIwsx288K3+rceO6h6vZbDaVLl1an332WZavDw4OzlZc999/v/2p5jeT1WeZVzw9PTMV89ndV4vFoiVLlmjLli363//+p1WrVmnAgAF6++23tWXLFvn6+qp06dL67bfftGrVKn3//ff6/vvvNW/ePPXp0yfLB8MBAAoeCm8AQJHh7u6uJ598UvPnz9fUqVO1YsWKLC//lq6e0bzR/v377Q8xq1SpkqSrlzLnpnC/Xvfu3bVgwQKtXbtWe/fulWEY9svMJSksLEyStG/fvkyv/euvv1SqVKlbnu0OCgpSfHy8w1hqaqpOnz59R3HfqHLlylqzZo2aN2+er4XxNWFhYdq1a5dsNptDofzXX3/Z11/777p16zK1IDt48GC23yun+3rvvffq3nvv1auvvqqFCxeqZ8+eWrRokZ566ilJkoeHhx599FE9+uijstlseuaZZzRr1iy9+OKLNz0TDwAoOLjHGwBQpPTu3VsXLlzQ4MGDlZiYqF69emU5b8WKFQ73aG/dulW//vqrHnroIUlS6dKl1apVK82aNSvLAvbvv//Odkxt2rRRiRIltHjxYi1evFiNGzd2uCw9JCRE9erV04IFCxwK6D179uiHH37Qww8/fMvtV65cWT/99JPD2OzZs296xju3unXrpoyMDL3yyiuZ1qWnp2cq/p3t4Ycf1pkzZ7R48WKH950xY4Z8fX3tT46PjIxUWlqaPvroI/s8m82m999/P9vvld19vXDhQqaWa/Xq1ZMkpaSkSLr6hPbrubm52Z9RcG0OAKBg44w3AKBIqV+/vurUqaMvv/xSNWvWVIMGDbKcV6VKFbVo0UJDhgxRSkqKoqKiVLJkSY0bN84+5/3331eLFi1Ut25dDRo0SJUqVdLZs2e1efNmnThxQr///nu2YrJarXr88ce1aNEiJSUl6a233so0580339RDDz2kpk2bauDAgfZ2YgEBAVn26L7eU089paefflpdunRR27Zt9fvvv2vVqlW3vZw7p1q2bKnBgwfrtdde02+//aZ27drJarXqwIED+vLLLzV9+nT94x//cOp7Xu+f//ynZs2apX79+mn79u2qWLGilixZoo0bNyoqKkp+fn6Srj44r3Hjxnruued08OBB1ahRQ19//bXOnz8vSQ4PlrvTfV2wYIE++OADde7cWZUrV9alS5f00Ucfyd/f3/4PJk899ZTOnz+vBx98UBUqVNDRo0c1Y8YM1atXz35/OgCgYKPwBgAUOX369NG4ceOyfKja9XPc3NwUFRWlc+fOqXHjxnrvvfcUEhJin1OrVi1t27ZNkydP1vz58xUXF6fSpUurfv36eumll3IUU/fu3fXxxx/LYrGoW7dumda3adNGK1eu1MSJE/XSSy/JarWqZcuWmjp1apYPbbveoEGDdOTIEc2ZM0crV67Ufffdp9WrV9sf5uZMM2fOVMOGDTVr1ixNmDBBxYoVU8WKFdWrVy81b97c6e93PW9vb61fv17PP/+8FixYoISEBFWvXl3z5s1Tv3797PPc3d317bffasSIEVqwYIHc3NzUuXNnTZw4Uc2bN5eXl1e23i87+9qyZUtt3bpVixYt0tmzZxUQEKDGjRvrs88+s+etV69emj17tj744APFx8erbNmy6t69uyZNmpTp3nIAQMFkMW68/gkAgEJu+vTpGjVqlGJiYjI9wTsmJkbh4eF68803NWbMGBdFCFdYsWKFOnfurF9++SXP/5EAAFC08M+oAIAixTAMzZkzRy1btsxUdKPouHz5ssNyRkaGZsyYIX9//5vefgAAQG5xqTkAoEhISkrS119/rXXr1mn37t366quvXB0SXOjZZ5/V5cuX1bRpU6WkpGjZsmXatGmTpkyZ4pInsgMACjcKbwBAkfD333/rySefVGBgoCZMmKDHHnvM1SHBhR588EG9/fbb+uabb3TlyhVVqVJFM2bM0LBhw1wdGgCgEOIebwAAAAAA8hD3eAMAAAAAkIcovAEAAAAAyEPc450Fm82mU6dOyc/PTxaLxdXhAAAAAABMxjAMXbp0SeXKlZOb263PaVN4Z+HUqVMKDQ11dRgAAAAAAJM7fvy4KlSocMs5FN5Z8PPzk3T1A/T393dxNDeXlpamH374Qe3atZPVanV1OLgOuTEvcmNu5Me8yI15kRtzIz/mRW7MrSDkJyEhQaGhofb68VYovLNw7fJyf39/0xfePj4+8vf3N+0fxqKK3JgXuTE38mNe5Ma8yI25kR/zIjfmVpDyk53bk3m4GgAAAAAAeYjCGwAAAACAPEThDQAAAABAHuIe7zuQkZGhtLQ0l71/WlqaihUrpitXrigjI8NlcRQUHh4et33MPwAAAAA4G4V3LhiGoTNnzig+Pt7lcZQtW1bHjx+n33g2uLm5KTw8XB4eHq4OBQAAAEARQuGdC9eK7tKlS8vHx8dlRa/NZlNiYqJ8fX05k3sbNptNp06d0unTp3XXXXfxDxUAAAAA8g2Fdw5lZGTYi+6SJUu6NBabzabU1FR5eXlReGdDcHCwTp06pfT0dNO3JAAAAABQeFCt5dC1e7p9fHxcHAly6tol5twPDwAAACA/UXjnEpcqFzzkDAAAAIArUHgDAAAAAJCHKLwBAAAAAMhDLi+8T548qV69eqlkyZLy9vZW3bp1tW3bNoc5e/fu1WOPPaaAgAAVL15cEREROnbs2E23OX/+fFksFocfLy+vvN6VAq9Vq1YOn1etWrX0wQcf3PF2+/XrlykfFotF7du3d5i3c+dOde/eXSEhIfL09FRYWJg6dOig//3vfzIMw2HuggULFBERIR8fH/n5+ally5b65ptv7jhWAAAAAHA2lxbeFy5cUPPmzWW1WvX999/rzz//1Ntvv62goCD7nEOHDqlFixaqUaOG1q9fr127dunFF1+8bSHt7++v06dP23+OHj2a17tTKAwaNEinT5/Wn3/+qW7dumno0KH6/PPPc7Wt1NRU+/+3b9/eIR+nT5922O5XX32le++9V4mJiVqwYIH27t2rlStXqnPnzvr3v/+tixcv2ueOGTNGgwcPVvfu3bVr1y5t3bpVLVq0UMeOHfXee+/lfucBAAAAIA+4tJ3Y1KlTFRoaqnnz5tnHwsPDHea88MILevjhh/XGG2/YxypXrnzbbVssFpUtW9Z5wRYCSUlJGjJkiJYtWyY/Pz+NGTNG//vf/1SvXj1FRUVJuvq09muf26RJk7Rw4UJ9/fXX6tGjh/71r39p+fLlOnHihMqWLauePXvqpZdesrfmmjRpklasWKFhw4bp1Vdf1dGjR2Wz2SRJnp6eN81HUlKSBg4cqEceeUTLli1zWFezZk0NHDjQfsZ7y5Ytevvtt/Xuu+/q2Weftc979dVXdeXKFY0ePVodO3ZUaGioUz87AAAAAMgtlxbeX3/9tSIjI9W1a1dt2LBB5cuX1zPPPKNBgwZJutqn+ttvv9W4ceMUGRmpnTt3Kjw8XOPHj1enTp1uue3ExESFhYXJZrOpQYMGmjJlimrXrp3l3JSUFKWkpNiXExISJF1tHXatfdg1aWlpMgxDNpvNXlQahqHLafnfosqrmJv9/a/FcitjxozRhg0btHz5cpUuXVovvPCCduzYoXvuucdhX67flre3t1JSUmSz2eTr66u5c+eqXLly2r17twYPHixfX1+NHTvW/tqDBw9qyZIlWrJkidzd3WWz2WQYxi1jXLlypeLi4jRmzJhb7odhGFq4cKF8fX01aNCgTHNHjRqladOmacmSJRoxYkSm11+LJS0tTe7u7rf9vO7EtT83N/75geuRG3MjP+Z1NPaSfouzyPb7yTz/OxQ5k5GRoV3kxrTIj3mRG3PLyMjQ0Uvm/k6Qk9hcWngfPnxYH374oUaPHq0JEyYoOjpaw4cPl4eHh/r27atz584pMTFRr7/+uv7zn/9o6tSpWrlypR5//HGtW7dOLVu2zHK71atX19y5c3X33Xfr4sWLeuutt9SsWTP98ccfqlChQqb5r732miZPnpxp/IcffsjUr7tYsWIqW7asEhMT7ZdSX07NUNNpW5zwieTM5tH3ytvDXZcuXbrt3MTERM2dO1ezZs1SRESEJGnGjBmqXbu2UlNTlZCQoPT0dPv/Z2RkaMmSJdq1a5d69eqlhIQEhzPMLVu21NChQ7Vo0SINHjxY0tV/wEhNTdV7772nUqVKSbr6jxhpaWn69ttv5e/v7xDTqFGj9Nxzz2n37t2SpHLlytn/0WPHjh167LHH7HM//vhjtW/fXn/++acqVqyoK1eu6MqVKw7b8/X1lZ+fn/bs2WPfzvVSU1N1+fJl/fTTT0pPT7/tZ+YMq1evzpf3Qc6RG3MjP+ay54JFC/a7KdXmLu3/w9XhIEvkxtzIj3mRGzOrG+SmcBN/J0hOTs72XJcW3jabTY0aNdKUKVMkSfXr19eePXs0c+ZM9e3b135Gs2PHjho1apQkqV69etq0aZNmzpx508K7adOmatq0qX25WbNmqlmzpmbNmqVXXnkl0/zx48dr9OjR9uWEhASFhoaqXbt2mYrFK1eu6Pjx4/L19bXfZ14sNX+KuBv5+vkqI+Wy/Pz8btuj+siRI0pNTVWrVq3s++Tv76/q1avLw8ND/v7+KlasmObMmaP//ve/Sk1Nlbu7u0aOHKlRo0bJzc1Nixcv1nvvvadDhw4pMTFR6enp8vf3t2/v2gPRKlWq5PDeVqtVrVq1yvSgthIlSsjf39/+Ofr5+dm31bRpU+3YsUOSMsXo5uaWKS/XWCwW+9wbXblyRd7e3rr//vvz/GF7aWlpWr16tdq2bWu/FB/mQG7MjfyYz+JtJzRny5+yGVIZb0MVggNv+zsH+cswDMVfiFdgELkxI/JjXuTG3AzDUImMC6b+TpDVyb6bcWnhHRISolq1ajmM1axZU0uXLpUklSpVSsWKFctyzi+//JLt97Farapfv74OHjyY5XpPT095enpm+bobk5yRkSGLxSI3Nze5uV291Lu4p1V/vhyZ7XicxdPdokspssdzK9fWXx/3Nde/vmfPnnrhhRfk7e2tkJAQ+/jmzZvVu3dvTZ48WZGRkQoICNCiRYv09ttv2+dYLBYVL148y+37+vqqWrVqWcZ2bfzAgQO69957JV29xP36+dfirl69ujZu3Kj09HR5eHg4bOfUqVNKSEhQ9erVs/w83NzcZLFYssxrXsnP90LOkBtzIz+uZxiG3llzQO+uPSBJerx+ObXwOKZHOzQhNyaTlpam7777Tg8/TG7MiPyYF7kxt2v5MfN3gpzE5dKnmjdv3lz79u1zGNu/f7/CwsIkSR4eHoqIiLjlnOzIyMjQ7t27FRIScudBZ8FiscjHo1i+/+TkX+YqV64sq9WqX3/91T524cIF7d+/32FeQECAqlSpovLlyzsUr5s2bVJYWJheeOEFNWrUSFWrVnXak+LbtWunEiVKaOrUqbed+8QTTygxMVGzZs3KtO6tt96S1WpVly5dnBIXABRVaRk2/WvpLnvRPfzBKnq9c225u7wJKQAABZNLz3iPGjVKzZo105QpU9StWzdt3bpVs2fP1uzZs+1zxo4dq+7du+v+++/XAw88oJUrV+p///uf1q9fb5/Tp08flS9fXq+99pok6eWXX9a9996rKlWqKD4+Xm+++aaOHj2qp556Kr930TR8fX01cOBAjR07ViVLlrQ/XO12Z8qvqVq1qo4dO6ZFixYpIiJC3377rZYvX57t909JSdGZM2ccxooVK6ZSpUrJ19dXH3/8sbp3765HHnlEw4cPV9WqVZWYmKiVK1dKkv2BF02bNtWIESM0duxYpaamqlOnTkpLS9Onn36q6dOnKyoqiieaA8AdSEpJ19CFO7R+399ys0ivdKqjnk3CTP1wGwAAzM6lhXdERISWL1+u8ePH6+WXX1Z4eLiioqLUs2dP+5zOnTtr5syZeu211zR8+HBVr15dS5cuVYsWLexzjh075lBAXrhwQYMGDdKZM2cUFBSkhg0batOmTZkuWS9q3nzzTSUmJurRRx+Vn5+fnnvuOYf+2Lfy2GOPadSoURo2bJhSUlL0yCOP6MUXX9SkSZOy9fqVK1dmuuKgevXq+uuvvyRdzfOmTZs0depU9enTR+fPn1dAQIAaNWqkRYsWqUOHDvbXRUVF6e6779YHH3ygf//733J3d1eDBg20YsUKPfroo9n7MAAAmcQmpmjA/GjtOnFRXlY3zejRQG1rlXF1WAAAFHgW41qDZNglJCQoICBAFy9ezPLhakeOHFF4eHieP6Drdmw2mxISEuTv75/tM9c3atWqlUMf78IsP3P3f/cMPWzae1KKKnJjbuTHdWJik9R33lYdjUtWkI9Vc/pFqMFdQfb15Ma8yI25kR/zIjfmVhDyc6u68UYuPeMNAABc77fj8RowP1rnk1IVWsJbC/o3VqVgX1eHBQBAoUHhDQBAEbZ271kNW7hTl9MyVLd8gOb2i1CwX+ZOHwAAIPcovIu46x9SBwAoWj7fekwvLN8tmyG1rBasD3o2UHFPvhoAAOBs/HYFAKCIubFH9z8aVtBrj9eVlX5hAADkCQpvAACKkLQMm15YvltfbDsh6WqP7lFtq8lisbg4MgAACi8K71yy2WyuDgE5xAP8ARR1N+vRDQAA8haFdw55eHjIzc1Np06dUnBwsDw8PFx2lsBmsyk1NVVXrlzJdTuxosIwDP3999+yWCymbUcAAHmJHt0AALgOhXcOubm5KTw8XKdPn9apU6dcGothGLp8+bK8vb25RDAbLBaLKlSoIHd3d1eHAgD56nY9ugEAQN6i8M4FDw8P3XXXXUpPT1dGRobL4khLS9NPP/2k+++/n7O42WC1Wim6ARQ59OgGAMD1KLxz6doly64seN3d3ZWeni4vLy8KbwBAJvToBgDAHCi8AQAohOjRDQCAefAbGACAQoQe3QAAmA+FNwAAhQQ9ugEAMCcKbwAACgF6dAMAYF4U3gAAFHD06AYAwNwovAEAKMDo0Q0AgPlReAMAUEDRoxsAgIKBwhsAgAKIHt0AABQcFN4AABQw9OgGAKBg4bc0AAAFBD26AQAomCi8AQAoAOjRDQBAwUXhDQCAydGjGwCAgo3CGwAAE6NHNwAABR+FNwAAJkWPbgAACgcKbwAATIge3QAAFB4U3gAAmAw9ugEAKFwovAEAMBF6dAMAUPjwmxwAABOgRzcAAIUXhTcAAC5Gj24AAAo3Cm8AAFyIHt0AABR+FN4AALgIPboBACgaKLwBAHABenQDAFB0UHgDAJDP6NENAEDRQuENAEA+okc3AABFD4U3AAD5hB7dAAAUTfy2BwAgj9GjGwCAoo3CGwCAPESPbgAAQOENAEAeoUc3AACQKLwBAMgT9OgGAADXUHgDAOBk9OgGAADXo/AGAMCJ6NENAABuROENAICT0KMbAABkhcIbAAAnoEc3AAC4Gb4RAABwB+jRDQAAbofCGwCAXKJHNwAAyA4KbwAAcoEe3QAAILsovAEAyCF6dAMAgJyg8AYAIAfo0Q0AAHKKwhsAgGyiRzcAAMgNCm8AALKBHt0AACC3XN7r5OTJk+rVq5dKliwpb29v1a1bV9u2bXOYs3fvXj322GMKCAhQ8eLFFRERoWPHjt1yu19++aVq1KghLy8v1a1bV999911e7gYAoBD7fOsxDfpkmy6nZahltWAt+ue9FN0AACDbXFp4X7hwQc2bN5fVatX333+vP//8U2+//baCgv7vXrlDhw6pRYsWqlGjhtavX69du3bpxRdflJeX1023u2nTJvXo0UMDBw7Uzp071alTJ3Xq1El79uzJj90CABQShmFo2ur9Gr9st2zG1R7dH/dtpOKeXDAGAACyz6XfHKZOnarQ0FDNmzfPPhYeHu4w54UXXtDDDz+sN954wz5WuXLlW253+vTpat++vcaOHStJeuWVV7R69Wq99957mjlzphP3AABQWNGjGwAAOItLC++vv/5akZGR6tq1qzZs2KDy5cvrmWee0aBBgyRJNptN3377rcaNG6fIyEjt3LlT4eHhGj9+vDp16nTT7W7evFmjR492GIuMjNSKFSuynJ+SkqKUlBT7ckJCgiQpLS1NaWlpd7aTeehabGaOsagiN+ZFbszNLPlJSknXiMW7tOFArNws0qRHa6pHRKjS09NdGpcrmSU3yIzcmBv5MS9yY24FIT85ic1iGIaRh7Hc0rXLxUePHq2uXbsqOjpaI0aM0MyZM9W3b1+dOXNGISEh8vHx0X/+8x898MADWrlypSZMmKB169apZcuWWW7Xw8NDCxYsUI8ePexjH3zwgSZPnqyzZ89mmj9p0iRNnjw50/jChQvl4+PjpL0FABQEl9KkWXvddTzJIqubob5VbapbwmW/KgEAgEklJyfrySef1MWLF+Xv73/LuS49422z2dSoUSNNmTJFklS/fn3t2bPHXnjbbDZJUseOHTVq1ChJUr169bRp0ybNnDnzpoV3To0fP97hDHlCQoJCQ0PVrl27236ArpSWlqbVq1erbdu2slqtrg4H1yE35kVuzM3V+Tkal6wBn2zX8aTLCvKxalav+qofGpjvcZiRq3ODmyM35kZ+zIvcmFtByM+1K6Wzw6WFd0hIiGrVquUwVrNmTS1dulSSVKpUKRUrVizLOb/88stNt1u2bNlMZ7bPnj2rsmXLZjnf09NTnp6Zn05rtVpNm+TrFZQ4iyJyY17kxtxckR96dGcPx455kRtzIz/mRW7Mzcz5yUlcLn2qefPmzbVv3z6Hsf379yssLEzS1UvGIyIibjknK02bNtXatWsdxlavXq2mTZs6KXIAQGGydu9Z9Zi9ReeTUlW3fICWDWlO0Q0AAJzGpWe8R40apWbNmmnKlCnq1q2btm7dqtmzZ2v27Nn2OWPHjlX37t11//332+/x/t///qf169fb5/Tp00fly5fXa6+9JkkaMWKEWrZsqbfffluPPPKIFi1apG3btjlsFwAA6WqP7heWX20X1rJasD7o2YB2YQAAwKlcesY7IiJCy5cv1+eff646derolVdeUVRUlHr27Gmf07lzZ82cOVNvvPGG6tatq48//lhLly5VixYt7HOOHTum06dP25ebNWumhQsXavbs2brnnnu0ZMkSrVixQnXq1MnX/QMAmBc9ugEAQH5x+beLDh06qEOHDrecM2DAAA0YMOCm668/+31N165d1bVr1zsNDwBQCNGjGwAA5CeXF94AAOSnpJR0DV24Q+v3/S03i/RKpzrq2eTmzw0BAAC4UxTeAIAiIzYxRQPmR2vXiYvysrppRo8GalurjKvDAgAAhRyFNwCgSIiJTVLfeVt1NC5ZQT5WzekXoQZ3Bbk6LAAAUARQeAMACj16dAMAAFei8AYAFGpr957VsIU7dTktQ3XLB2huvwgF+3m6OiwAAFCEUHgDAAotenQDAAAz4NsHAKDQMQxD76w5oHfXHpB0tUf3a4/XldXdzcWRAQCAoojCGwBQqNCjGwAAmA2FNwCg0KBHNwAAMCMKbwBAoUCPbgAAYFYU3gCAAo8e3QAAwMwovAEABRo9ugEAgNlReAMACix6dAMAgIKAwhsAUCDRoxsAABQUfEMBABQo9OgGAAAFDYU3AKDAoEc3AAAoiCi8AQAFAj26AQBAQUXhDQAwPXp0AwCAgozCGwBgavToBgAABR2FNwDAtOjRDQAACgMKbwCAKdGjGwAAFBYU3gAA01m87YRe+vpPenQDAIBCgW8xAADTMAxD3x1306rNf0qiRzcAACgcKLwBAKaQlmHThBV/atWJq0U2PboBAEBhQeENAHC563t0W2Ro8mO11KdZJVeHBQAA4BQU3gAAl7qxR3evSmnqERHq6rAAAACchpvmAAAuExObpC4fbtKuExcV5GPVJ/0bqW4Jw9VhAQAAOBWFNwDAJX47Hq/HP9yko3HJCi3hraVDmql+aKCrwwIAAHA6LjUHAOS7m/XoTktLc3VoAAAATkfhDQDIV59vPaYXlu+mRzcAACgy+KYDAMgXhmHonTUH9O7aA5Lo0Q0AAIoOCm8AQJ5Ly7DpheW79cW2E5Lo0Q0AAIoWCm8AQJ66vke3m0V6pVMd9WwS5uqwAAAA8g2FNwAgz9zYo3tGjwZqW6uMq8MCAADIVxTeAIA8ERObpL7ztupoXLKCfKya0y9CDe4KcnVYAAAA+Y7CGwDgdL8dj9eA+dE6n5Sq0BLeWtC/sSoF+7o6LAAAAJeg8AYAONX1PbrrlPfX3H4RKu3n5eqwAAAAXIbCGwDgNNf36L7///fo9qVHNwAAKOL4NgQAuGM39uju0qCCXu9Cj24AAACJwhsAcIdu7NH97INVNJoe3QAAAHYU3gCAXKNHNwAAwO1ReAMAcoUe3QAAANlD4Q0AyLEbe3R/3DdCDcPo0Q0AAJAVCm8AQI7QoxsAACBnKLwBANlGj24AAICco/AGAGQLPboBAAByh29MAIBbokc3AADAnaHwBgDcFD26AQAA7hyFNwAgS/ToBgAAcA4KbwBAJvToBgAAcB6X36B38uRJ9erVSyVLlpS3t7fq1q2rbdu22df369dPFovF4ad9+/a33OakSZMyvaZGjRp5vSsAUCjExCapy4ebtOvERQX5WPXZU/dSdAMAANwBl57xvnDhgpo3b64HHnhA33//vYKDg3XgwAEFBQU5zGvfvr3mzZtnX/b09LzttmvXrq01a9bYl4sV4+Q+ANzOb8fjNXB+tOLo0Q0AAOA0Lq1Gp06dqtDQUIeiOjw8PNM8T09PlS1bNkfbLlasWI5fAwBF2Y9/ndXQz+jRDQAA4GwuvdT866+/VqNGjdS1a1eVLl1a9evX10cffZRp3vr161W6dGlVr15dQ4YMUVxc3G23feDAAZUrV06VKlVSz549dezYsbzYBQAoFBZtPaZBn2zX5bQM3V8tWIv+2ZSiGwAAwElcesb78OHD+vDDDzV69GhNmDBB0dHRGj58uDw8PNS3b19JVy8zf/zxxxUeHq5Dhw5pwoQJeuihh7R582a5u7tnud0mTZpo/vz5ql69uk6fPq3Jkyfrvvvu0549e+Tn55dpfkpKilJSUuzLCQkJkqS0tDSlpaXlwZ47x7XYzBxjUUVuzIvcODIMQzPWHdKMdYclSZ3rl9OrHWvJ6ma45DMiP+ZFbsyL3Jgb+TEvcmNuBSE/OYnNYhiGkYex3JKHh4caNWqkTZs22ceGDx+u6Ohobd68OcvXHD58WJUrV9aaNWvUunXrbL1PfHy8wsLCNG3aNA0cODDT+kmTJmny5MmZxhcuXCgfH59s7g0AFCwZhvTFYTdtOXf14qd25W16ONQmWnQDAADcXnJysp588kldvHhR/v7+t5zr0jPeISEhqlWrlsNYzZo1tXTp0pu+plKlSipVqpQOHjyY7cI7MDBQ1apV08GDB7NcP378eI0ePdq+nJCQoNDQULVr1+62H6ArpaWlafXq1Wrbtq2sVqurw8F1yI15kZurklPTNXzxLm05Fys3izTp0ZrqERHq6rDIj4mRG/MiN+ZGfsyL3JhbQcjPtSuls8OlhXfz5s21b98+h7H9+/crLCzspq85ceKE4uLiFBISku33SUxM1KFDh9S7d+8s13t6emb5pHSr1WraJF+voMRZFJEb8yrKuYlNTNHA+dv1u4l7dBfl/JgduTEvcmNu5Me8yI25mTk/OYnLpQ9XGzVqlLZs2aIpU6bo4MGDWrhwoWbPnq2hQ4dKulowjx07Vlu2bFFMTIzWrl2rjh07qkqVKoqMjLRvp3Xr1nrvvffsy2PGjNGGDRsUExOjTZs2qXPnznJ3d1ePHj3yfR8BwEyu9ej+nR7dAAAA+calZ7wjIiK0fPlyjR8/Xi+//LLCw8MVFRWlnj17SpLc3d21a9cuLViwQPHx8SpXrpzatWunV155xeEM9aFDhxQbG2tfPnHihHr06KG4uDgFBwerRYsW2rJli4KDg/N9HwHALOjRDQAA4BouLbwlqUOHDurQoUOW67y9vbVq1arbbiMmJsZhedGiRc4IDQAKDXp0AwAAuI7LC28AQN5atPWYXlixRxk2Q/dXC9YHPRvI15O//gEAAPIL37wAoJAyDENRaw5o+toDkqQuDSro9S51ZXV36eM9AAAAihwKbwAohNIzbHph+R4t3nZckvTsg1U0um01WWjSDQAAkO8ovAGgkElOTdfQz3Zo3b6/5WaRXulURz2b3LxNIwAAAPIWhTcAFCJXe3RHm7pHNwAAQFFD4Q0AhURMbJL6ztuqo3HJCvKx6uO+EWoYFuTqsAAAAIo8Cm8AKATo0Q0AAGBeFN4AUMDRoxsAAMDcKLwBoACjRzcAAID58e0MAAogenQDAAAUHBTeAFDA0KMbAACgYKHwBoAChB7dAAAABU+ur0lMT0/XmjVrNGvWLF26dEmSdOrUKSUmJjotOADA/4lNTFGP2Vu0bt/f8rK6aVbvRhTdAAAABUCuzngfPXpU7du317Fjx5SSkqK2bdvKz89PU6dOVUpKimbOnOnsOAGgSKNHNwAAQMGVqzPeI0aMUKNGjXThwgV5e3vbxzt37qy1a9c6LTgAwNUe3V0+3KSjcckKLeGtpUOaUXQDAAAUILk64/3zzz9r06ZN8vDwcBivWLGiTp486ZTAAAD06AYAACgMclV422w2ZWRkZBo/ceKE/Pz87jgoAAA9ugEAAAqLXF1q3q5dO0VFRdmXLRaLEhMTNXHiRD388MPOig0AiiTDMPTO6v16ftluZdgMdWlQQXP6NqLoBgAAKKBy9S3u7bffVmRkpGrVqqUrV67oySef1IEDB1SqVCl9/vnnzo4RAIoMenQDAAAUPrkqvCtUqKDff/9dixYt0q5du5SYmKiBAweqZ8+eDg9bAwBkHz26AQAACqdcX7dYrFgx9erVy5mxAECRFZuYooHzo/X7iYvysrppRo8GalurjKvDAgAAgBPkuvA+deqUfvnlF507d042m81h3fDhw+84MAAoKujRDQAAULjlqvCeP3++Bg8eLA8PD5UsWdLh3kOLxULhDQDZ9NvxeA2cH624pFSFlvDWgv6NVSnY19VhAQAAwIlyVXi/+OKLeumllzR+/Hi5ueXqwegAUOTRoxsAAKBoyFXhnZycrCeeeIKiGwByiR7dAAAARUeuKueBAwfqyy+/dHYsAFDo0aMbAACg6MnVN73XXntNHTp00MqVK1W3bl1ZrVaH9dOmTXNKcABQmNCjGwAAoGjKdeG9atUqVa9eXZIyPVwNAOCIHt0AAABFV64K77fffltz585Vv379nBwOABQ+9OgGAAAo2nJVeHt6eqp58+bOjgUACh16dAMAACBXD1cbMWKEZsyY4exYAKBQ+e14vLp8uElH45IVWsJbS4c0o+gGAAAognJ1xnvr1q368ccf9c0336h27dqZHq62bNkypwQHAAUVPboBAABwTa4K78DAQD3++OPOjgUACgV6dAMAAOB6ufomOG/ePGfHAQAFnmEYilpzQNPXHpAkdWlQQa93qSure67u6gEAAEAhwSkYAHACenQDAADgZnJVeIeHh9/yy+Thw4dzHRAAFDT06AYAAMCt5KrwHjlypMNyWlqadu7cqZUrV2rs2LHOiAsACgR6dAMAAOB2clV4jxgxIsvx999/X9u2bbujgACgoKBHNwAAALLDqU/8eeihh7R06VJnbhIATIke3QAAAMgupz5cbcmSJSpRooQzNwkApkOPbgAAAORErgrv+vXrOzxczTAMnTlzRn///bc++OADpwUHAGZDj24AAADkVK6+LXbq1Mlh2c3NTcHBwWrVqpVq1KjhjLgAwFTo0Q0AAIDcylXhPXHiRGfHAQCmRY9uAAAA3IlsF94JCQnZ3qi/v3+uggEAs6FHNwAAAO5UtgvvwMDA257dMQxDFotFGRkZdxwYALgaPboBAADgDNkuvNetW5eXcQCAqdCjGwAAAM6S7cK7ZcuWeRkHAJjGb8fjNXB+tOKSUhVawlsL+jdWpWBfV4cFAACAAirXPXDi4+M1Z84c7d27V5JUu3ZtDRgwQAEBAU4LDgDyGz26AQAA4Gy56oOzbds2Va5cWe+8847Onz+v8+fPa9q0aapcubJ27Njh7BgBIF8s2npMgz7ZrstpGbq/WrAW/bMpRTcAAADuWK7OeI8aNUqPPfaYPvroIxUrdnUT6enpeuqppzRy5Ej99NNPTg0SAPISPboBAACQl3JVeG/bts2h6JakYsWKady4cWrUqJHTggOAvEaPbgAAAOS1XJ3O8ff317FjxzKNHz9+XH5+fjna1smTJ9WrVy+VLFlS3t7eqlu3rrZt22Zf369fP1ksFoef9u3b33a777//vipWrCgvLy81adJEW7duzVFcAAq/5NR0DfpkmxZvOy43i/Rq5zp6rl11im4AAAA4Va7OeHfv3l0DBw7UW2+9pWbNmkmSNm7cqLFjx6pHjx7Z3s6FCxfUvHlzPfDAA/r+++8VHBysAwcOKCjIsWVP+/btNW/ePPuyp6fnLbe7ePFijR49WjNnzlSTJk0UFRWlyMhI7du3T6VLl87BngIorOISUzT4s9/o0Q0AAIA8l6vC+6233pLFYlGfPn2Unp4uSbJarRoyZIhef/31bG9n6tSpCg0NdSiqw8PDM83z9PRU2bJls73dadOmadCgQerfv78kaebMmfr22281d+5cPf/889neDoDC6e/LUrePturY+cv06AYAAECey1Xh7eHhoenTp+u1117ToUOHJEmVK1eWj49Pjrbz9ddfKzIyUl27dtWGDRtUvnx5PfPMMxo0aJDDvPXr16t06dIKCgrSgw8+qP/85z8qWbJklttMTU3V9u3bNX78ePuYm5ub2rRpo82bN2f5mpSUFKWkpNiXExISJElpaWlKS0vL0T7lp2uxmTnGoorcmNf2mDhF7XFXYvplVQjy1tw+DRReqji5MgmOHfMiN+ZFbsyN/JgXuTG3gpCfnMRmMQzDyOkbfPrpp3r88cdzXGjfyMvrapue0aNHq2vXroqOjtaIESM0c+ZM9e3bV5K0aNEi+fj4KDw8XIcOHdKECRPk6+urzZs3y93dPdM2T506pfLly2vTpk1q2rSpfXzcuHHasGGDfv3110yvmTRpkiZPnpxpfOHChXe8jwDM448LFs3f76ZUm0UVihsaXCND/h6ujgoAAAAFUXJysp588kldvHhR/v7+t5ybq8I7ODhYly9f1mOPPaZevXopMjIyyyL4djw8PNSoUSNt2rTJPjZ8+HBFR0ff9Oz04cOHVblyZa1Zs0atW7fOtD43hXdWZ7xDQ0MVGxt72w/QldLS0rR69Wq1bdtWVqvV1eHgOuTGfL7YdkIv/W+vMmyGagTY9N+nWyrQ19vVYeEGHDvmRW7Mi9yYG/kxL3JjbgUhPwkJCSpVqlS2Cu9cXWp++vRprVy5Up9//rm6desmHx8fde3aVT179rQ/bC07QkJCVKtWLYexmjVraunSpTd9TaVKlVSqVCkdPHgwy8K7VKlScnd319mzZx3Gz549e9P7xD09PbN8YJvVajVtkq9XUOIsisiN693Yo7tz/XK6z+OYAn29yY2JceyYF7kxL3JjbuTHvMiNuZk5PzmJK1ftxIoVK6YOHTros88+07lz5/TOO+8oJiZGDzzwgCpXrpzt7TRv3lz79u1zGNu/f7/CwsJu+poTJ04oLi5OISEhWa738PBQw4YNtXbtWvuYzWbT2rVrHc6AAyj80jNsen7pbnvR/eyDVTS1c2255+pvPgAAACB37vjrp4+PjyIjI/XQQw+patWqiomJyfZrR40apS1btmjKlCk6ePCgFi5cqNmzZ2vo0KGSpMTERI0dO1ZbtmxRTEyM1q5dq44dO6pKlSqKjIy0b6d169Z677337MujR4/WRx99pAULFmjv3r0aMmSIkpKS7E85B1D40aMbAAAAZpGrS82lqzeSL1++XJ999pnWrl2r0NBQ9ejRQ0uWLMn2NiIiIrR8+XKNHz9eL7/8ssLDwxUVFaWePXtKktzd3bVr1y4tWLBA8fHxKleunNq1a6dXXnnF4dLwQ4cOKTY21r7cvXt3/f3333rppZd05swZ1atXTytXrlSZMvToBYqC2MQUDZwfTY9uAAAAmEKuCu8nnnhC33zzjXx8fNStWze9+OKLub6Mu0OHDurQoUOW67y9vbVq1arbbiOrs+zDhg3TsGHDchUTgIIrJjZJfedt1dG4ZHp0AwAAwBRyVXi7u7vriy++yPXTzAEgL/x2PF4D50crLilVoSW8taB/Y1UK9nV1WAAAACjiclx4p6Wl6cyZM6patSpFNwDT+PGvsxr62U5dTstQnfL+mtsvQqX9vFwdFgAAAJDzwttqtWrXrl15EQsA5Mqircf0woo9yrAZur9asD7o2UC+nrl+hAUAAADgVLl6qnmvXr00Z84cZ8cCADliGIbeWb1fzy/brQyboS4NKmhO30YU3QAAADCVXH07TU9P19y5c7VmzRo1bNhQxYsXd1g/bdo0pwQHADeTnmHTC8v3aPG245Ku9uge3bYa7cIAAABgOrkqvPfs2aMGDRpIkvbv3++wji+9APJacmq6hn62Q+v2/S03i/RKpzrq2STM1WEBAAAAWcpV4b1u3TpnxwEA2UKPbgAAABQ0ubrH+5qDBw9q1apVunz5sqSr91sCQF6JiU1Slw836fcTFxXkY9VnT91L0Q0AAADTy1XhHRcXp9atW6tatWp6+OGHdfr0aUnSwIED9dxzzzk1QACQrvbo7vLhJh2NS1ZoCW8tHdJMDcOCXB0WAAAAcFu5KrxHjRolq9WqY8eOycfHxz7evXt3rVy50mnBAYB0tUd3j9lbFJeUqjrl/bV0SDNVCvZ1dVgAAABAtuTqHu8ffvhBq1atUoUKFRzGq1atqqNHjzolMACQ6NENAACAgi9X316TkpIcznRfc/78eXl6et5xUABgGIai1hzQ9LUHJEldGlTQ613qyup+R4+mAAAAAPJdrr7B3nffffrkk0/syxaLRTabTW+88YYeeOABpwUHoGhKz7Dp+aW77UX3sw9W0Vtd76boBgAAQIGUqzPeb7zxhlq3bq1t27YpNTVV48aN0x9//KHz589r48aNzo4RQBFCj24AAAAUNrk6fVSnTh3t379fLVq0UMeOHZWUlKTHH39cO3fuVOXKlZ0dI4AiIjYxRT1mb9G6fX/Ly+qmWb0bUXQDAACgwMv1E4oCAgL0wgsvODMWAEVYTGyS+s7bqqNxyQryserjvhG0CwMAAEChkOvC+8KFC5ozZ4727t0rSapVq5b69++vEiVKOC04AEXDb8fjNXB+tOKSUhVawlsL+jemXRgAAAAKjVxdav7TTz+pYsWKevfdd3XhwgVduHBB7777rsLDw/XTTz85O0YAhRg9ugEAAFDY5eqM99ChQ9W9e3d9+OGHcnd3lyRlZGTomWee0dChQ7V7926nBgmgcKJHNwAAAIqCXJ3xPnjwoJ577jl70S1J7u7uGj16tA4ePOi04AAUToZh6J3V+/X8st3KsBnq0qCC5vRtRNENAACAQilXhXeDBg3s93Zfb+/evbrnnnvuOCgAhRc9ugEAAFDU5Or00vDhwzVixAgdPHhQ9957ryRpy5Ytev/99/X6669r165d9rl33323cyIFUODRoxsAAABFUa4K7x49ekiSxo0bl+U6i8UiwzBksViUkZFxZxECKBRiE1M0cH60fj9xUV5WN83o0UBta5VxdVgAAABAnstV4X3kyBFnxwGgEKNHNwAAAIqyXBXepUqVUvHixZ0dC4BCiB7dAAAAKOpy9TSjMmXKaMCAAfrll1+cHQ+AQoQe3QAAAEAuC+9PP/1U58+f14MPPqhq1arp9ddf16lTp5wdG4ACbNHWYxr0yXZdTsvQ/dWCteifTVXaz8vVYQEAAAD5LleFd6dOnbRixQqdPHlSTz/9tBYuXKiwsDB16NBBy5YtU3p6urPjBFBA0KMbAAAAcHRHjXODg4M1evRo7dq1S9OmTdOaNWv0j3/8Q+XKldNLL72k5ORkZ8UJoACgRzcAAACQ2R2dgjp79qwWLFig+fPn6+jRo/rHP/6hgQMH6sSJE5o6daq2bNmiH374wVmxAjAxenQDAAAAWctV4b1s2TLNmzdPq1atUq1atfTMM8+oV69eCgwMtM9p1qyZatas6aw4AZgYPboBAACAm8tV4d2/f3898cQT2rhxoyIiIrKcU65cOb3wwgt3FBwA86NHNwAAAHBrOSq8ExISJEl//fWXvY/3tbHr+fv7y9vbWxMnTnRCiADMih7dAAAAwO3lqPAODAyUxWK56XrDMGSxWJSRkXHHgQEwtx//Oquhn+3U5bQM1Snvr7n9ImgXBgAAAGQhR4X3unXr7P9vGIYefvhhffzxxypfvrzTAwNgXou2HtMLK/Yow2bo/mrB+qBnA9qFAQAAADeRo2/KLVu2dFh2d3fXvffeq0qVKjk1KADmZBiGotYcsLcL69Kggl7vUpd2YQAAAMAtcIoKQLakZ9j0wvI9WrztuKSrPbpHt612y9tPAAAAAFB4A8gGenQDAAAAuXfHhTdnu4DCjR7dAAAAwJ3JUeH9+OOPOyxfuXJFTz/9tL212DXLli2788gAuBw9ugEAAIA7l6PCOyAgwGG5V69eTg0GgHnQoxsAAABwjhwV3vPmzcurOACYCD26AQAAAOfh4WoAHNCjGwAAAHAuvk0DkESPbgAAACCvUHgDyNSje9gDVfRcO3p0AwAAAM5A4Q0Uccmp6Rq2cKd+/Ouc3CzSyx3rqNe99OgGAAAAnIXCGyjCru/R7VnMTTN61Fe72mVdHRYAAABQqFB4A0UUPboBAACA/EHhDRRBvx+P14D/36O7QpC3FgxorMr06AYAAADyBIU3UMRc36O7djl/zetPj24AAAAgL7m8T9DJkyfVq1cvlSxZUt7e3qpbt662bduW5dynn35aFotFUVFRt9zmpEmTZLFYHH5q1KiRB9EDBcvi6GMa9Ml2XU7L0H1VS2nx4KYU3QAAAEAec+kZ7wsXLqh58+Z64IEH9P333ys4OFgHDhxQUFDm+0yXL1+uLVu2qFy5ctnadu3atbVmzRr7crFinNxH0WUYhqavPaCoNVd7dD/eoLymdrmbHt0AAABAPnBpNTp16lSFhoZq3rx59rHw8PBM806ePKlnn31Wq1at0iOPPJKtbRcrVkxly/J0ZiA9w6Z/r9ijRdH06AYAAABcwaWnu77++ms1atRIXbt2VenSpVW/fn199NFHDnNsNpt69+6tsWPHqnbt2tne9oEDB1SuXDlVqlRJPXv21LFjx5wdPmB6yanp+ud/t2tR9HG5WaT/dKqjMZHVKboBAACAfOTSM96HDx/Whx9+qNGjR2vChAmKjo7W8OHD5eHhob59+0q6ela8WLFiGj58eLa326RJE82fP1/Vq1fX6dOnNXnyZN13333as2eP/Pz8Ms1PSUlRSkqKfTkhIUGSlJaWprS0tDvcy7xzLTYzx1hUmSE3cYkp+uenO7XrZII8i7kpqtvdalOzdJH/82KG3ODmyI95kRvzIjfmRn7Mi9yYW0HIT05isxiGYeRhLLfk4eGhRo0aadOmTfax4cOHKzo6Wps3b9b27dv1yCOPaMeOHfZ7uytWrKiRI0dq5MiR2X6f+Ph4hYWFadq0aRo4cGCm9ZMmTdLkyZMzjS9cuFA+Pj453zHAxf6+LM3c667YFIuKFzM0qEaGwjP/mxMAAACAXEpOTtaTTz6pixcvyt/f/5ZzXXrGOyQkRLVq1XIYq1mzppYuXSpJ+vnnn3Xu3Dnddddd9vUZGRl67rnnFBUVpZiYmGy9T2BgoKpVq6aDBw9muX78+PEaPXq0fTkhIUGhoaFq167dbT9AV0pLS9Pq1avVtm1bWa1WV4eD67gyN7tOXNTkT3fofEqaKgR6aU6fhqoUXDxfYzAzjhtzIz/mRW7Mi9yYG/kxL3JjbgUhP9eulM4OlxbezZs31759+xzG9u/fr7CwMElS79691aZNG4f1kZGR6t27t/r375/t90lMTNShQ4fUu3fvLNd7enrK09Mz07jVajVtkq9XUOIsivI7N/Tozj6OG3MjP+ZFbsyL3Jgb+TEvcmNuZs5PTuJyaeE9atQoNWvWTFOmTFG3bt20detWzZ49W7Nnz5YklSxZUiVLlnR4jdVqVdmyZVW9enX7WOvWrdW5c2cNGzZMkjRmzBg9+uijCgsL06lTpzRx4kS5u7urR48e+bdzQD5bHH1ME5bvUYbN0H1VS+nDXg3l60kbPQAAAMDVXPqtPCIiQsuXL9f48eP18ssvKzw8XFFRUerZs2eOtnPo0CHFxsbal0+cOKEePXooLi5OwcHBatGihbZs2aLg4GBn7wLgcvToBgAAAMzN5afDOnTooA4dOmR7flb3dd84tmjRojuMCigY6NENAAAAmJ/LC28AuZOcmq5hC3fqx7/Oyc0ivdyxjnrdG+bqsAAAAADcgMIbKIBiE1M0cH60fj9xUZ7F3DSjR321q13W1WEBAAAAyAKFN1DAxMQmqe+8rToal6wgH6s+7huhhmFBrg4LAAAAwE1QeAMFyO/H4zVgfrTiklJVIchbCwY0VuVgX1eHBQAAAOAWKLyBAoIe3QAAAEDBROENFAD06AYAAAAKLr65AyZGj24AAACg4KPwBkyKHt0AAABA4UDhDZgQPboBAACAwoPCGzAZenQDAAAAhQuFN2Ai9OgGAAAACh8Kb8Ak6NENAAAAFE4U3oAJ0KMbAAAAKLwovAEXo0c3AAAAULjx7R5wEXp0AwAAAEUDhTfgAvToBgAAAIoOCm8gn9GjGwAAAChaKLyBfESPbgAAAKDoofAG8gk9ugEAAICiicIbyAf06AYAAACKLgpvII/RoxsAAAAo2ii8gTz05fYTevHrvfToBgAAAIowKgAgDxiGoe+PW7Ry85+S6NENAAAAFGUU3oCTpWfY9O+v/tTKE+6S6NENAAAAFHUU3oATXd+j2yJDkx6tpb7NK7k6LAAAAAAuROENOMmNPbp7V07Tk41DXR0WAAAAABfjhlPACWJik9Tlw036/cRFBflY9d/+jVS3hOHqsAAAAACYAIU3cId+Px6vLh9u0tG4ZFUI8taSIc1U/65AV4cFAAAAwCS41By4Azfr0Z2Wlubq0AAAAACYBIU3kEuLo49pwvI99OgGAAAAcEtUCUAOGYah6WsPKGrNAUn06AYAAABwaxTeQA6kZ9j07xV7tCj6uCR6dAMAAAC4PQpvIJuu79HtZpFe7lhHve4Nc3VYAAAAAEyOwhvIhht7dM/oUV/tapd1dVgAAAAACgAKb+A2YmKT1HfeVh2NS1aQj1Uf941Qw7AgV4cFAAAAoICg8AZu4ffj8RowP1pxSamqEOStBQMaq3Kwr6vDAgAAAFCAUHgDN3GzHt0AAAAAkBMU3kAW6NENAAAAwFmoJIDr0KMbAAAAgLNReAP/Hz26AQAAAOQFCm9A9OgGAAAAkHcovFHk0aMbAAAAQF6i8EaRRo9uAAAAAHmNwhtFFj26AQAAAOQHCm8USfToBgAAAJBfKLxR5NCjGwAAAEB+otpAkUGPbgAAAACuQOGNIoEe3QAAAABchcIbhR49ugEAAAC4EoU3CjV6dAMAAABwNQpvFFr06AYAAABgBi5/qtTJkyfVq1cvlSxZUt7e3qpbt662bduW5dynn35aFotFUVFRt93u+++/r4oVK8rLy0tNmjTR1q1bnRw5zOz34/Hq8uEmHY1LVoUgby0Z0oyiGwAAAIBLuLTwvnDhgpo3by6r1arvv/9ef/75p95++20FBWUukJYvX64tW7aoXLlyt93u4sWLNXr0aE2cOFE7duzQPffco8jISJ07dy4vdgMm8+NfZ/XE7C2KS0pV7XL+WvZMM1UO9nV1WAAAAACKKJcW3lOnTlVoaKjmzZunxo0bKzw8XO3atVPlypUd5p08eVLPPvusPvvsM1mt1ttud9q0aRo0aJD69++vWrVqaebMmfLx8dHcuXPzaldgEoujj2nQJ9t1OS1D91UtpcWDm6q0n5erwwIAAABQhLn0Hu+vv/5akZGR6tq1qzZs2KDy5cvrmWee0aBBg+xzbDabevfurbFjx6p27dq33WZqaqq2b9+u8ePH28fc3NzUpk0bbd68OcvXpKSkKCUlxb6ckJAgSUpLS1NaWlpudy/PXYvNzDHmF8Mw9N66w3p33SFJUud6IXq1U21Z3QyXfD7kxrzIjbmRH/MiN+ZFbsyN/JgXuTG3gpCfnMRmMQzDyMNYbsnL6+qZyNGjR6tr166Kjo7WiBEjNHPmTPXt21eS9Nprr2ndunVatWqVLBaLKlasqJEjR2rkyJFZbvPUqVMqX768Nm3apKZNm9rHx40bpw0bNujXX3/N9JpJkyZp8uTJmcYXLlwoHx8fJ+wp8lKGIX152E2bz129gKNdeZseDrWJFt0AAAAA8kpycrKefPJJXbx4Uf7+/rec69Iz3jabTY0aNdKUKVMkSfXr19eePXvshff27ds1ffp07dixQ5Y8rKLGjx+v0aNH25cTEhIUGhqqdu3a3fYDdKW0tDStXr1abdu2zdYl+IVRcmq6Rizepc3nYuVmkSZ2qKknG4e6OixyY2LkxtzIj3mRG/MiN+ZGfsyL3JhbQcjPtSuls8OlhXdISIhq1arlMFazZk0tXbpUkvTzzz/r3Llzuuuuu+zrMzIy9NxzzykqKkoxMTGZtlmqVCm5u7vr7NmzDuNnz55V2bJZ92/29PSUp6dnpnGr1WraJF+voMTpbFd7dG83dY/uopqbgoDcmBv5MS9yY17kxtzIj3mRG3Mzc35yEpdLH67WvHlz7du3z2Fs//79CgsLkyT17t1bu3bt0m+//Wb/KVeunMaOHatVq1ZluU0PDw81bNhQa9eutY/ZbDatXbvW4dJzFGwxsUnq8uEm/X7iooJ8rFo46F7TFd0AAAAAILn4jPeoUaPUrFkzTZkyRd26ddPWrVs1e/ZszZ49W5JUsmRJlSxZ0uE1VqtVZcuWVfXq1e1jrVu3VufOnTVs2DBJV+8Z79u3rxo1aqTGjRsrKipKSUlJ6t+/f/7tHPLM78fjNWB+tOKSUlUhyFsLBjSmXRgAAAAA03Jp4R0REaHly5dr/PjxevnllxUeHq6oqCj17NkzR9s5dOiQYmNj7cvdu3fX33//rZdeeklnzpxRvXr1tHLlSpUpU8bZu4B89uNfZzX0s526nJah2uX8Na9/BO3CAAAAAJiaSwtvSerQoYM6dOiQ7flZ3ded1diwYcPsZ8BROCyOPqYJy/cow2bovqql9GGvhvL1dPkfYQAAAAC4JaoWmJ5hGJq+9oCi1hyQJD3eoLymdrlbVneXPqIAAAAAALKFwhumlp5h079X7NGi6OOSpGEPVNFz7arlaXs5AAAAAHAmCm+YVnJquoYt3Kkf/zonN4v0csc66nVvmKvDAgAAAIAcofCGKV3t0R1t6h7dAAAAAJAdFN4wnZjYJPWdt1VH45IV5GPVx30j1DAsyNVhAQAAAECuUHjDVOjRDQAAAKCwofCGadCjGwAAAEBhROENU6BHNwAAAIDCisoGLkWPbgAAAACFHYU3XIYe3QAAAACKAgpvuAQ9ugEAAAAUFRTeyHf06AYAAABQlFB4I1/RoxsAAABAUUPhjXxDj24AAAAARRGFN/IFPboBAAAAFFUU3shz9OgGAAAAUJRR/SDP0KMbAAAAACi8kUfo0Q0AAAAAV1F4w+no0Q0AAAAA/4fCG05Fj24AAAAAcEThDaehRzcAAAAAZEbhDaegRzcAAAAAZI3CG3eMHt0AAAAAcHMU3rgj9OgGAAAAgFujQkKu0KMbAAAAALKHwhs5Ro9uAAAAAMg+Cm/kCD26AQAAACBnKLyRbfToBgAAAICco/BGttCjGwAAAAByh8Ibt0WPbgAAAADIPQpv3BI9ugEAAADgzlB446bo0Q0AAAAAd44qCpnQoxsAAAAAnIfCGw7o0Q0AAAAAzkXhDTt6dAMAAACA81F4QxI9ugEAAAAgr1B4gx7dAAAAAJCHKLyLOHp0AwAAAEDeovAuwujRDQAAAAB5j8K7iKJHNwAAAADkDyqtIsYwDL279qDeWbNfEj26AQAAACCvUXgXIekZNr341R59vpUe3QAAAACQXyi8i4jk1HQ9u3Cn1tKjGwAAAADyFYV3ERCXmKIBC7bp9+Px9OgGAAAAgHxG4V3IHY1LUt+5WxVDj24AAAAAcAkK70KMHt0AAAAA4HoU3oXUur/O6ZnPdtCjGwAAAABcjMK7EPoi+rjGL99Nj24AAAAAMAGqsUKEHt0AAAAAYD4U3oUEPboBAAAAwJxcfir05MmT6tWrl0qWLClvb2/VrVtX27Zts6+fNGmSatSooeLFiysoKEht2rTRr7/+esttTpo0SRaLxeGnRo0aeb0rLpOcmq7B/92uz7cel5tF+k+nOhoTWZ2iGwAAAABMwKVnvC9cuKDmzZvrgQce0Pfff6/g4GAdOHBAQUH/1+6qWrVqeu+991SpUiVdvnxZ77zzjtq1a6eDBw8qODj4ptuuXbu21qxZY18uVqxwntxPTJN6z9umXScS6NENAAAAACbk0mp06tSpCg0N1bx58+xj4eHhDnOefPJJh+Vp06Zpzpw52rVrl1q3bn3TbRcrVkxlyxbuAvTo+WS9s8ddsVcS6NENAAAAACbl0kvNv/76azVq1Ehdu3ZV6dKlVb9+fX300Uc3nZ+amqrZs2crICBA99xzzy23feDAAZUrV06VKlVSz549dezYMWeH71K/H49Xt9m/KvaKRRUCvbRkSDOKbgAAAAAwIZee8T58+LA+/PBDjR49WhMmTFB0dLSGDx8uDw8P9e3b1z7vm2++0RNPPKHk5GSFhIRo9erVKlWq1E2326RJE82fP1/Vq1fX6dOnNXnyZN13333as2eP/Pz8Ms1PSUlRSkqKfTkhIUGSlJaWprS0NCfusfO89+MBnU9KU4Xihj4b0EDlAj1NG2tRdC0X5MR8yI25kR/zIjfmRW7MjfyYF7kxt4KQn5zEZjEMw8jDWG7Jw8NDjRo10qZNm+xjw4cPV3R0tDZv3mwfS0pK0unTpxUbG6uPPvpIP/74o3799VeVLl06W+8THx+vsLAwTZs2TQMHDsy0ftKkSZo8eXKm8YULF8rHxycXe5b3rmRI3x1z08N32eTl7upoAAAAAKBoSU5O1pNPPqmLFy/K39//lnNdesY7JCREtWrVchirWbOmli5d6jBWvHhxValSRVWqVNG9996rqlWras6cORo/fny23icwMFDVqlXTwYMHs1w/fvx4jR492r6ckJCg0NBQtWvX7rYfoCs9mpam1atXq23btrJara4OB9dJIzemRW7MjfyYF7kxL3JjbuTHvMiNuRWE/Fy7Ujo7XFp4N2/eXPv27XMY279/v8LCwm75OpvN5nBp+O0kJibq0KFD6t27d5brPT095enpmWncarWaNsnXKyhxFkXkxrzIjbmRH/MiN+ZFbsyN/JgXuTE3M+cnJ3G59OFqo0aN0pYtWzRlyhQdPHhQCxcu1OzZszV06FBJVy8xnzBhgrZs2aKjR49q+/btGjBggE6ePKmuXbvat9O6dWu999579uUxY8Zow4YNiomJ0aZNm9S5c2e5u7urR48e+b6PAAAAAICizaVnvCMiIrR8+XKNHz9eL7/8ssLDwxUVFaWePXtKktzd3fXXX39pwYIFio2NVcmSJRUREaGff/5ZtWvXtm/n0KFDio2NtS+fOHFCPXr0UFxcnIKDg9WiRQtt2bLlln2/AQAAAADICy4tvCWpQ4cO6tChQ5brvLy8tGzZsttuIyYmxmF50aJFzggNAAAAAIA75tJLzQEAAAAAKOwovAEAAAAAyEMU3gAAAAAA5CEKbwAAAAAA8hCFNwAAAAAAeYjCGwAAAACAPEThDQAAAABAHqLwBgAAAAAgD1F4AwAAAACQhyi8AQAAAADIQxTeAAAAAADkoWKuDsCMDMOQJCUkJLg4kltLS0tTcnKyEhISZLVaXR0OrkNuzIvcmBv5MS9yY17kxtzIj3mRG3MrCPm5Vi9eqx9vhcI7C5cuXZIkhYaGujgSAAAAAICZXbp0SQEBAbecYzGyU54XMTabTadOnZKfn58sFourw7mphIQEhYaG6vjx4/L393d1OLgOuTEvcmNu5Me8yI15kRtzIz/mRW7MrSDkxzAMXbp0SeXKlZOb263v4uaMdxbc3NxUoUIFV4eRbf7+/qb9w1jUkRvzIjfmRn7Mi9yYF7kxN/JjXuTG3Myen9ud6b6Gh6sBAAAAAJCHKLwBAAAAAMhDFN4FmKenpyZOnChPT09Xh4IbkBvzIjfmRn7Mi9yYF7kxN/JjXuTG3Apbfni4GgAAAAAAeYgz3gAAAAAA5CEKbwAAAAAA8hCFNwAAAAAAeYjC20Tef/99VaxYUV5eXmrSpIm2bt16y/lffvmlatSoIS8vL9WtW1ffffedw3rDMPTSSy8pJCRE3t7eatOmjQ4cOJCXu1Bo5SQ3H330ke677z4FBQUpKChIbdq0yTS/X79+slgsDj/t27fP690otHKSn/nz52f67L28vBzmcOw4T05y06pVq0y5sVgseuSRR+xzOHac46efftKjjz6qcuXKyWKxaMWKFbd9zfr169WgQQN5enqqSpUqmj9/fqY5Of09hqzlND/Lli1T27ZtFRwcLH9/fzVt2lSrVq1ymDNp0qRMx06NGjXycC8Kp5zmZv369Vn+vXbmzBmHeRw7dy6nucnq94nFYlHt2rXtczhunOO1115TRESE/Pz8VLp0aXXq1En79u277esKW61D4W0Sixcv1ujRozVx4kTt2LFD99xzjyIjI3Xu3Lks52/atEk9evTQwIEDtXPnTnXq1EmdOnXSnj177HPeeOMNvfvuu5o5c6Z+/fVXFS9eXJGRkbpy5Up+7VahkNPcrF+/Xj169NC6deu0efNmhYaGql27djp58qTDvPbt2+v06dP2n88//zw/dqfQyWl+JMnf39/hsz969KjDeo4d58hpbpYtW+aQlz179sjd3V1du3Z1mMexc+eSkpJ0zz336P3338/W/CNHjuiRRx7RAw88oN9++00jR47UU0895VDc5eZYRNZymp+ffvpJbdu21Xfffaft27frgQce0KOPPqqdO3c6zKtdu7bDsfPLL7/kRfiFWk5zc82+ffscPvvSpUvb13HsOEdOczN9+nSHnBw/flwlSpTI9DuH4+bObdiwQUOHDtWWLVu0evVqpaWlqV27dkpKSrrpawplrWPAFBo3bmwMHTrUvpyRkWGUK1fOeO2117Kc361bN+ORRx5xGGvSpIkxePBgwzAMw2azGWXLljXefPNN+/r4+HjD09PT+Pzzz/NgDwqvnObmRunp6Yafn5+xYMEC+1jfvn2Njh07OjvUIimn+Zk3b54REBBw0+1x7DjPnR4777zzjuHn52ckJibaxzh2nE+SsXz58lvOGTdunFG7dm2Hse7duxuRkZH25TvNN7KWnfxkpVatWsbkyZPtyxMnTjTuuece5wWGbOVm3bp1hiTjwoULN53DseN8uTluli9fblgsFiMmJsY+xnGTN86dO2dIMjZs2HDTOYWx1uGMtwmkpqZq+/btatOmjX3Mzc1Nbdq00ebNm7N8zebNmx3mS1JkZKR9/pEjR3TmzBmHOQEBAWrSpMlNt4nMcpObGyUnJystLU0lSpRwGF+/fr1Kly6t6tWra8iQIYqLi3Nq7EVBbvOTmJiosLAwhYaGqmPHjvrjjz/s6zh2nMMZx86cOXP0xBNPqHjx4g7jHDv573a/c5yRbziPzWbTpUuXMv3eOXDggMqVK6dKlSqpZ8+eOnbsmIsiLHrq1aunkJAQtW3bVhs3brSPc+yYx5w5c9SmTRuFhYU5jHPcON/FixclKdPfUdcrjLUOhbcJxMbGKiMjQ2XKlHEYL1OmTKZ7gK45c+bMLedf+29OtonMcpObG/3rX/9SuXLlHP5iaN++vT755BOtXbtWU6dO1YYNG/TQQw8pIyPDqfEXdrnJT/Xq1TV37lx99dVX+vTTT2Wz2dSsWTOdOHFCEseOs9zpsbN161bt2bNHTz31lMM4x45r3Ox3TkJCgi5fvuyUvyvhPG+99ZYSExPVrVs3+1iTJk00f/58rVy5Uh9++KGOHDmi++67T5cuXXJhpIVfSEiIZs6cqaVLl2rp0qUKDQ1Vq1attGPHDknO+Z6BO3fq1Cl9//33mX7ncNw4n81m08iRI9W8eXPVqVPnpvMKY61TzNUBAIXZ66+/rkWLFmn9+vUOD/B64okn7P9ft25d3X333apcubLWr1+v1q1buyLUIqNp06Zq2rSpfblZs2aqWbOmZs2apVdeecWFkeF6c+bMUd26ddW4cWOHcY4d4NYWLlyoyZMn66uvvnK4j/ihhx6y///dd9+tJk2aKCwsTF988YUGDhzoilCLhOrVq6t69er25WbNmunQoUN655139N///teFkeF6CxYsUGBgoDp16uQwznHjfEOHDtWePXuK5L3ynPE2gVKlSsnd3V1nz551GD979qzKli2b5WvKli17y/nX/puTbSKz3OTmmrfeekuvv/66fvjhB9199923nFupUiWVKlVKBw8evOOYi5I7yc81VqtV9evXt3/2HDvOcSe5SUpK0qJFi7L1pYZjJ3/c7HeOv7+/vL29nXIs4s4tWrRITz31lL744otMl2jeKDAwUNWqVePYcYHGjRvbP3eOHdczDENz585V79695eHhccu5HDd3ZtiwYfrmm2+0bt06VahQ4ZZzC2OtQ+FtAh4eHmrYsKHWrl1rH7PZbFq7dq3DmbnrNW3a1GG+JK1evdo+Pzw8XGXLlnWYk5CQoF9//fWm20RmucmNdPUpi6+88opWrlypRo0a3fZ9Tpw4obi4OIWEhDgl7qIit/m5XkZGhnbv3m3/7Dl2nONOcvPll18qJSVFvXr1uu37cOzkj9v9znHGsYg78/nnn6t///76/PPPHVrw3UxiYqIOHTrEseMCv/32m/1z59hxvQ0bNujgwYPZ+sdejpvcMQxDw4YN0/Lly/Xjjz8qPDz8tq8plLWOq5/uhqsWLVpkeHp6GvPnzzf+/PNP45///KcRGBhonDlzxjAMw+jdu7fx/PPP2+dv3LjRKFasmPHWW28Ze/fuNSZOnGhYrVZj9+7d9jmvv/66ERgYaHz11VfGrl27jI4dOxrh4eHG5cuX833/CrKc5ub11183PDw8jCVLlhinT5+2/1y6dMkwDMO4dOmSMWbMGGPz5s3GkSNHjDVr1hgNGjQwqlataly5csUl+1iQ5TQ/kydPNlatWmUcOnTI2L59u/HEE08YXl5exh9//GGfw7HjHDnNzTUtWrQwunfvnmmcY8d5Ll26ZOzcudPYuXOnIcmYNm2asXPnTuPo0aOGYRjG888/b/Tu3ds+//Dhw4aPj48xduxYY+/evcb7779vuLu7GytXrrTPuV2+kX05zc9nn31mFCtWzHj//fcdfu/Ex8fb5zz33HPG+vXrjSNHjhgbN2402rRpY5QqVco4d+5cvu9fQZbT3LzzzjvGihUrjAMHDhi7d+82RowYYbi5uRlr1qyxz+HYcY6c5uaaXr16GU2aNMlymxw3zjFkyBAjICDAWL9+vcPfUcnJyfY5RaHWofA2kRkzZhh33XWX4eHhYTRu3NjYsmWLfV3Lli2Nvn37Osz/4osvjGrVqhkeHh5G7dq1jW+//dZhvc1mM1588UWjTJkyhqenp9G6dWtj3759+bErhU5OchMWFmZIyvQzceJEwzAMIzk52WjXrp0RHBxsWK1WIywszBg0aBC/YO9ATvIzcuRI+9wyZcoYDz/8sLFjxw6H7XHsOE9O/17766+/DEnGDz/8kGlbHDvOc63F0Y0/1/LRt29fo2XLlpleU69ePcPDw8OoVKmSMW/evEzbvVW+kX05zU/Lli1vOd8wrrZ/CwkJMTw8PIzy5csb3bt3Nw4ePJi/O1YI5DQ3U6dONSpXrmx4eXkZJUqUMFq1amX8+OOPmbbLsXPncvP3Wnx8vOHt7W3Mnj07y21y3DhHVnmR5PB7pCjUOhbDMIw8O50OAAAAAEARxz3eAAAAAADkIQpvAAAAAADyEIU3AAAAAAB5iMIbAAAAAIA8ROENAAAAAEAeovAGAAAAACAPUXgDAAAAAJCHKLwBAAAAAMhDFN4AAMBpKlasqKioKFeHAQCAqVB4AwBQQPXr10+dOnWSJLVq1UojR47Mt/eeP3++AgMDM41HR0frn//8Z77FAQBAQVDM1QEAAADzSE1NlYeHR65fHxwc7MRoAAAoHDjjDQBAAdevXz9t2LBB06dPl8VikcViUUxMjCRpz549euihh+Tr66syZcqod+/eio2Ntb+2VatWGjZsmEaOHKlSpUopMjJSkjRt2jTVrVtXxYsXV2hoqJ555hklJiZKktavX6/+/fvr4sWL9vebNGmSpMyXmh87dkwdO3aUr6+v/P391a1bN509e9a+ftKkSapXr57++9//qmLFigoICNATTzyhS5cu5e2HBgBAPqLwBgCggJs+fbqaNm2qQYMG6fTp0zp9+rRCQ0MVHx+vBx98UPXr19e2bdu0cuVKnT17Vt26dXN4/YIFC+Th4aGNGzdq5syZkiQ3Nze9++67+uOPP7RgwQL9+OOPGjdunCSpWbNmioqKkr+/v/39xowZkykum82mjh076vz589qwYYNWr16tw4cPq3v37g7zDh06pBUrVuibb77RN998ow0bNuj111/Po08LAID8x6XmAAAUcAEBAfLw8JCPj4/Kli1rH3/vvfdUv359TZkyxT42d+5chYaGav/+/apWrZokqWrVqnrjjTcctnn9/eIVK1bUf/7zHz399NP64IMP5OHhoYCAAFksFof3u9HatWu1e/duHTlyRKGhoZKkTz75RLVr11Z0dLQiIiIkXS3Q58+fLz8/P0lS7969tXbtWr366qt39sEAAGASnPEGAKCQ+v3337Vu3Tr5+vraf2rUqCHp6lnmaxo2bJjptWvWrFHr1q1Vvnx5+fn5qXfv3oqLi1NycnK233/v3r0KDQ21F92SVKtWLQUGBmrv3r32sYoVK9qLbkkKCQnRuXPncrSvAACYGWe8AQAopBITE/Xoo49q6tSpmdaFhITY/7948eIO62JiYtShQwcNGTJEr776qkqUKKFffvlFAwcOVGpqqnx8fJwap9VqdVi2WCyy2WxOfQ8AAFyJwhsAgELAw8NDGRkZDmMNGjTQ0qVLVbFiRRUrlv1f+du3b5fNZtPbb78tN7erF8d98cUXt32/G9WsWVPHjx/X8ePH7We9//zzT8XHx6tWrVrZjgcAgIKOS80BACgEKlasqF9//VUxMTGKjY2VzWbT0KFDdf78efXo0UPR0dE6dOiQVq1apf79+9+yaK5SpYrS0tI0Y8YMHT58WP/973/tD127/v0SExO1du1axcbGZnkJeps2bVS3bl317NlTO3bs0NatW9WnTx+1bNlSjRo1cvpnAACAWVF4AwBQCIwZM0bu7u6qVauWgoODdezYMZUrV04bN25URkaG2rVrp7p162rkyJEKDAy0n8nOyj333KNp06Zp6tSpqlOnjj777DO99tprDnOaNWump59+Wt27d1dwcHCmh7NJVy8Z/+qrrxQUFKT7779fbdq0UaVKlbR48WKn7z8AAGZmMQzDcHUQAAAAAAAUVpzxBgAAAAAgD1F4AwAAAACQhyi8AQAAAADIQxTeAAAAAADkIQpvAAAAAADyEIU3AAAAAAB5iMIbAAAAAIA8ROENAAAAAEAeovAGAAAAACAPUXgDAAAAAJCHKLwBAAAAAMhDFN4AAAAAAOSh/wdWzr67URJplQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[qParEGO] Batch 2: HV = 66.4947, time = 40.26s\n"
     ]
    }
   ],
   "source": [
    "hvs_qparego = []\n",
    "train_x_qparego, train_obj_true_qparego = generate_initial_data(n=2 * (problem.dim + 1))\n",
    "mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_true_qparego)\n",
    "\n",
    "bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qparego)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "hvs_qparego.append(volume)\n",
    "\n",
    "iteration = 0\n",
    "while iteration < N_BATCH and check_hv_early_stopping(hvs_qparego):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    fit_gpytorch_mll(mll_qparego)\n",
    "    sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "\n",
    "    new_x, new_obj = optimize_qnparego_and_get_observation(model_qparego, train_x_qparego, train_obj_true_qparego, sampler)\n",
    "    train_x_qparego = torch.cat([train_x_qparego, new_x])\n",
    "    train_obj_true_qparego = torch.cat([train_obj_true_qparego, new_obj])\n",
    "\n",
    "    bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qparego)\n",
    "    volume = bd.compute_hypervolume().item()\n",
    "    hvs_qparego.append(volume)\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "\n",
    "    #Plot the hv\n",
    "    plot_hv_progress({\"qParEGO\": hvs_qparego})\n",
    "\n",
    "    # Inside the while loop, after computing HV\n",
    "    pareto_mask = is_non_dominated(train_obj_true_qparego)\n",
    "    pareto_front = train_obj_true_qparego[pareto_mask]\n",
    "\n",
    "    log_rows.append({\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"suggested_candidates\": new_x.cpu().numpy().tolist(),\n",
    "        \"objective_values\": new_obj.cpu().numpy().tolist(),\n",
    "        \"hypervolume\": volume,\n",
    "        \"time_sec\": round(t1 - t0, 3),\n",
    "        \"pareto_front\": pareto_front.cpu().numpy().tolist(),\n",
    "    })\n",
    "    \n",
    "    mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_true_qparego)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[qParEGO] Batch {iteration + 1}: HV = {volume:.4f}, time = {t1 - t0:.2f}s\")\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "save_pareto_plot_2d(train_obj_true_qparego.cpu(), filename=f\"../plots/final_pareto_front_qparego_{run_number}.png\")\n",
    "\n",
    "for row in log_rows:\n",
    "    row.update(metadata)\n",
    "    row[\"n_evaluations\"] = (row[\"iteration\"]) * metadata[\"batch_size\"]\n",
    "\n",
    "df = pd.DataFrame(log_rows)\n",
    "df.to_csv(f'../logs/qparego_run_log{run_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rows = []\n",
    "\n",
    "# Metadata for the current run\n",
    "metadata = {\n",
    "    \"dataset\": \"dataOK\",\n",
    "    \"gp_model\": \"SingleGP\",  # or \"MultiGP\"\n",
    "    \"acquisition\": \"qQEHVI\",\n",
    "    \"seed\": seed,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"run_id\": run_number,  # if looping over multiple configurations\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 569728,
     "status": "ok",
     "timestamp": 1744974896827,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "9f6uxP-Vi06n",
    "outputId": "236ff7d7-4fa1-4502-cc0e-08c7eae52c5f"
   },
   "outputs": [],
   "source": [
    "hvs_qehvi = []\n",
    "train_x_qehvi, train_obj_true_qehvi = train_x_qparego, train_obj_true_qparego\n",
    "mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_true_qehvi)\n",
    "\n",
    "bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qehvi)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "hvs_qehvi.append(volume)\n",
    "\n",
    "iteration = 0\n",
    "while iteration < N_BATCH and check_hv_early_stopping(hvs_qehvi):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    fit_gpytorch_mll(mll_qehvi)\n",
    "    sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "\n",
    "    new_x, new_obj = optimize_qehvi_and_get_observation(model_qehvi, train_x_qehvi, train_obj_true_qehvi, sampler)\n",
    "    train_x_qehvi = torch.cat([train_x_qehvi, new_x])\n",
    "    train_obj_true_qehvi = torch.cat([train_obj_true_qehvi, new_obj])\n",
    "\n",
    "    bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qehvi)\n",
    "    volume = bd.compute_hypervolume().item()\n",
    "    hvs_qehvi.append(volume)\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "\n",
    "    #plot the hv\n",
    "    plot_hv_progress({\"qEHVI\": hvs_qehvi})\n",
    "\n",
    "    # Inside the while loop, after computing HV\n",
    "    pareto_mask = is_non_dominated(train_obj_true_qehvi)\n",
    "    pareto_front = train_obj_true_qehvi[pareto_mask]\n",
    "\n",
    "    log_rows.append({\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"suggested_candidates\": new_x.cpu().numpy().tolist(),\n",
    "        \"objective_values\": new_obj.cpu().numpy().tolist(),\n",
    "        \"hypervolume\": volume,\n",
    "        \"time_sec\": round(t1 - t0, 3),\n",
    "        \"pareto_front\": pareto_front.cpu().numpy().tolist(),\n",
    "    })\n",
    "    \n",
    "    mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_true_qehvi)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[qEHVI] Batch {iteration + 1}: HV = {volume:.4f}, time = {t1 - t0:.2f}s\")\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "save_pareto_plot_2d(train_obj_true_qehvi, filename=f\"../plots/final_pareto_front_qhevi_{run_number}.png\")\n",
    "\n",
    "for row in log_rows:\n",
    "    row.update(metadata)\n",
    "    row[\"n_evaluations\"] = (row[\"iteration\"]) * metadata[\"batch_size\"]\n",
    "\n",
    "df = pd.DataFrame(log_rows)\n",
    "df.to_csv(f'../logs/qehvi_run_log{run_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rows = []\n",
    "\n",
    "# Metadata for the current run\n",
    "metadata = {\n",
    "    \"dataset\": \"dataOK\",\n",
    "    \"gp_model\": \"SingleGP\",  # or \"MultiGP\"\n",
    "    \"acquisition\": \"qQNEHVI\",\n",
    "    \"seed\": seed,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"run_id\": run_number,  # if looping over multiple configurations\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 314641,
     "status": "ok",
     "timestamp": 1744975211473,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "s6ZMZeeMi2ln",
    "outputId": "051aa16c-10dd-4656-8faf-2c754c790217"
   },
   "outputs": [],
   "source": [
    "hvs_qnehvi = []\n",
    "train_x_qnehvi, train_obj_true_qnehvi = train_x_qparego, train_obj_true_qparego\n",
    "mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_true_qnehvi)\n",
    "\n",
    "bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qnehvi)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "hvs_qnehvi.append(volume)\n",
    "\n",
    "iteration = 0\n",
    "while iteration < N_BATCH and check_hv_early_stopping(hvs_qnehvi):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    fit_gpytorch_mll(mll_qnehvi)\n",
    "    sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
    "\n",
    "    new_x, new_obj = optimize_qnehvi_and_get_observation(model_qnehvi, train_x_qnehvi, train_obj_true_qnehvi, sampler)\n",
    "    train_x_qnehvi = torch.cat([train_x_qnehvi, new_x])\n",
    "    train_obj_true_qnehvi = torch.cat([train_obj_true_qnehvi, new_obj])\n",
    "\n",
    "    bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qnehvi)\n",
    "    volume = bd.compute_hypervolume().item()\n",
    "    hvs_qnehvi.append(volume)\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "\n",
    "    #plot the hv\n",
    "    plot_hv_progress({\"qNEHVI\": hvs_qnehvi})\n",
    "\n",
    "    # Inside the while loop, after computing HV\n",
    "    pareto_mask = is_non_dominated(train_obj_true_qnehvi)\n",
    "    pareto_front = train_obj_true_qnehvi[pareto_mask]\n",
    "\n",
    "    log_rows.append({\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"suggested_candidates\": new_x.cpu().numpy().tolist(),\n",
    "        \"objective_values\": new_obj.cpu().numpy().tolist(),\n",
    "        \"hypervolume\": volume,\n",
    "        \"time_sec\": round(t1 - t0, 3),\n",
    "        \"pareto_front\": pareto_front.cpu().numpy().tolist(),\n",
    "    })\n",
    "    \n",
    "    mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_true_qnehvi)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[qNEHVI] Batch {iteration + 1}: HV = {volume:.4f}, time = {t1 - t0:.2f}s\")\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "save_pareto_plot_2d(train_obj_true_qnehvi, filename=f\"../plots/final_pareto_front_qnehvi_{run_number}.png\")\n",
    "\n",
    "for row in log_rows:\n",
    "    row.update(metadata)\n",
    "    row[\"n_evaluations\"] = (row[\"iteration\"]) * metadata[\"batch_size\"]\n",
    "\n",
    "df = pd.DataFrame(log_rows)\n",
    "df.to_csv(f'../logs/qnehvi_run_log{run_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rows = []\n",
    "\n",
    "# Metadata for the current run\n",
    "metadata = {\n",
    "    \"dataset\": \"dataOK\",\n",
    "    \"gp_model\": \"SingleGP\",  # or \"MultiGP\"\n",
    "    \"acquisition\": \"Sobol\",\n",
    "    \"seed\": seed,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"run_id\": run_number,  # if looping over multiple configurations\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 86881,
     "status": "ok",
     "timestamp": 1744975298353,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "tOwHyrPli4dh",
    "outputId": "f33e562a-1bed-4253-f8c5-aa2f16f3a8b1"
   },
   "outputs": [],
   "source": [
    "hvs_random = []\n",
    "train_x_random, train_obj_true_random = train_x_qparego, train_obj_true_qparego\n",
    "\n",
    "bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_random)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "hvs_random.append(volume)\n",
    "\n",
    "iteration = 0\n",
    "while iteration < N_BATCH and check_hv_early_stopping(hvs_random):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    new_x, new_obj = generate_initial_data(n=BATCH_SIZE)\n",
    "    train_x_random = torch.cat([train_x_random, new_x])\n",
    "    train_obj_true_random = torch.cat([train_obj_true_random, new_obj])\n",
    "\n",
    "    bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_random)\n",
    "    volume = bd.compute_hypervolume().item()\n",
    "    hvs_random.append(volume)\n",
    "    \n",
    "    t1 = time.monotonic()\n",
    "\n",
    "    #plot the hv\n",
    "    plot_hv_progress({\"Random\": hvs_random})\n",
    "\n",
    "    # Inside the while loop, after computing HV\n",
    "    pareto_mask = is_non_dominated(train_obj_true_random)\n",
    "    pareto_front = train_obj_true_random[pareto_mask]\n",
    "\n",
    "    log_rows.append({\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"suggested_candidates\": new_x.cpu().numpy().tolist(),\n",
    "        \"objective_values\": new_obj.cpu().numpy().tolist(),\n",
    "        \"hypervolume\": volume,\n",
    "        \"time_sec\": round(t1 - t0, 3),\n",
    "        \"pareto_front\": pareto_front.cpu().numpy().tolist(),\n",
    "    })\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[Random] Batch {iteration + 1}: HV = {volume:.4f}, time = {t1 - t0:.2f}s\")\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "save_pareto_plot_2d(train_obj_true_random, filename=f\"../plots/final_pareto_front_sobol_{run_number}.png\")\n",
    "\n",
    "\n",
    "for row in log_rows:\n",
    "    row.update(metadata)\n",
    "    row[\"n_evaluations\"] = (row[\"iteration\"]) * metadata[\"batch_size\"]\n",
    "\n",
    "df = pd.DataFrame(log_rows)\n",
    "df.to_csv(f'../logs/sobol_run_log{run_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXGqYsCJoTrd"
   },
   "source": [
    "## Best points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744975298370,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "Yzz9tjU_iv-k"
   },
   "outputs": [],
   "source": [
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "\n",
    "# Get boolean mask for non-dominated points\n",
    "pareto_mask = is_non_dominated(train_obj_true_qparego)\n",
    "\n",
    "# Extract Pareto front objectives\n",
    "pareto_Y = train_obj_true_qparego[pareto_mask]\n",
    "pareto_X = train_x_qparego[pareto_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1744975298380,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "n6JduW6zi0Yq",
    "outputId": "b9a43ac0-911a-45e5-9de3-48964d943089"
   },
   "outputs": [],
   "source": [
    "for y in pareto_Y:\n",
    "    print(f\"Objective1 = {y[0].item():.4f}, Objective2 = {y[1].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0jj0Rtojs_k"
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1744975298931,
     "user": {
      "displayName": "AFINA NUROVA",
      "userId": "15890823735226591957"
     },
     "user_tz": -120
    },
    "id": "4Z8JjBD1w_XF",
    "outputId": "9e4dac71-bcf2-43bd-d76a-07149c215380"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(23, 7), sharex=True, sharey=True)\n",
    "algos = [\"Sobol\", \"qNParEGO\", \"qEHVI\", \"qNEHVI\"]\n",
    "cm = plt.get_cmap(\"viridis\")\n",
    "\n",
    "# Set which objective dimensions to plot\n",
    "obj_x, obj_y = 0, 1\n",
    "\n",
    "# Put all train_obj references in a list\n",
    "train_objs = [\n",
    "    train_obj_true_random,\n",
    "    train_obj_true_qparego,\n",
    "    train_obj_true_qehvi,\n",
    "    train_obj_true_qnehvi,\n",
    "]\n",
    "\n",
    "# Colorbar tracking\n",
    "all_batch_numbers = []\n",
    "\n",
    "for i, train_obj in enumerate(train_objs):\n",
    "    obj_np = train_obj.cpu().numpy()\n",
    "\n",
    "    # Determine how many design points were added over time\n",
    "    n_total = obj_np.shape[0]\n",
    "    n_initial = 2 * (problem.dim + 1)\n",
    "    n_added = n_total - n_initial\n",
    "    n_batches = n_added // BATCH_SIZE\n",
    "\n",
    "    # Generate per-point batch number for this algo\n",
    "    batch_number = np.concatenate([\n",
    "        np.zeros(n_initial),  # Initial design\n",
    "        np.repeat(np.arange(1, n_batches + 1), BATCH_SIZE)\n",
    "    ])\n",
    "    all_batch_numbers.append(batch_number)\n",
    "\n",
    "    # Plot\n",
    "    sc = axes[i].scatter(\n",
    "        obj_np[:, obj_x],\n",
    "        obj_np[:, obj_y],\n",
    "        c=batch_number,\n",
    "        cmap=cm,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    axes[i].set_title(algos[i])\n",
    "    axes[i].set_xlabel(f\"Objective {obj_x + 1}\")\n",
    "\n",
    "axes[0].set_ylabel(f\"Objective {obj_y + 1}\")\n",
    "\n",
    "# Normalize colorbar across *all* batch numbers\n",
    "full_batch = np.concatenate(all_batch_numbers)\n",
    "norm = plt.Normalize(full_batch.min(), full_batch.max())\n",
    "sm = ScalarMappable(norm=norm, cmap=cm)\n",
    "sm.set_array([])\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.ax.set_title(\"Iteration\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMFFXpPp4EbpQlhNGhDWlR8",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1MgZtLQiz5M3iXd4n71Jqq0BnT7Rh3_Yy",
     "timestamp": 1744449761299
    },
    {
     "file_id": "12IzXl48GdBXRRR1FrzkzhTC4fPy8wfN8",
     "timestamp": 1743615472512
    },
    {
     "file_id": "153LStAQ-qO6B5S9iIw1tChCLLacem0u0",
     "timestamp": 1741107768524
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
